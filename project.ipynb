{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["Euib_RmfOiT3","aBD3fRknjEj6","9TpGEYTblzQc","W2Nk5Gp7hUGA","QQcLNRgD6SaW","gLHjV3q28LNr","pewE01Ca4BG0","QXXrs_PjAHrN","HLns9_iJvgA3","gYAkkmYGC4OG","N3mNpQT1li9C","p7AK6T9Picu-","eosqWqRRJLsZ","DjBsdz9mKbZg"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["The skeleton of the code is from [2]. The major part of Reformer and Informer are from the official code release [3][4].\n","\n","[1]\tGitHub - kzl/decision-transformer: Official codebase for Decision Transformer: Reinforcement Learning via Sequence Modeling.\n","\n","[2]\tGitHub - nikhilbarhate99/min-decision-transformer: Minimal implementation of Decision Transformer: Reinforcement Learning via Sequence Modeling in PyTorch for mujoco control tasks in OpenAI gym\n","\n","[3]\tGitHub - lucidrains/reformer-pytorch: Reformer, the efficient Transformer, in Pytorch\n","\n","[4]\tGitHub - zhouhaoyi/Informer2020: The GitHub repository for the paper \"Informer\" accepted by AAAI 2021.\n","\n","[5]\tGitHub - thuml/Autoformer: About Code release for \"Autoformer: Decomposition \tTransformers with Auto-Correlation for Long-Term Series Forecasting\" (NeurIPS 2021)"],"metadata":{"id":"wyPEHbBzceUu"}},{"cell_type":"markdown","metadata":{"id":"Euib_RmfOiT3"},"source":["## install mujoco-py and D4RL\n","\n","* **Restart Runtime** after running this block to complete D4RL setup\n"]},{"cell_type":"code","metadata":{"id":"DAGCHznQs2bI"},"source":["\n","###### libs for install ######\n","\n","\n","!sudo apt-get update\n","!sudo apt-get install gcc\n","\n","!sudo apt-get build-dep mesa\n","!sudo apt-get install llvm-dev\n","!sudo apt-get install freeglut3 freeglut3-dev\n","\n","!sudo apt-get install python3-dev\n","\n","!sudo apt-get install build-essential\n","\n","!sudo apt install curl git libgl1-mesa-dev libgl1-mesa-glx libglew-dev \\\n","        libosmesa6-dev software-properties-common net-tools unzip vim \\\n","        virtualenv wget xpra xserver-xorg-dev libglfw3-dev patchelf\n","\n","#!sudo apt-get install -y libglew-dev\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"23eqLoV_orip"},"source":["\n","###### mujoco setup ######\n","\n","\n","#!wget https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz\n","\n","!wget https://roboti.us/download/mujoco200_linux.zip\n","\n","!wget https://roboti.us/file/mjkey.txt\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WcXVniz_p4RN"},"source":["\n","!mkdir /root/.mujoco\n","\n","### mujoco 210\n","#!tar -xf mujoco210-linux-x86_64.tar.gz -C /.mujoco/\n","#!ls -alh /.mujoco/mujoco210\n","\n","### mujoco 200\n","!unzip mujoco200_linux.zip -d /root/.mujoco/\n","!cp -r /root/.mujoco/mujoco200_linux /root/.mujoco/mujoco200\n","\n","!mv mjkey.txt /root/.mujoco/\n","\n","!cp -r /root/.mujoco/mujoco200/bin/* /usr/lib/\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-X3JOM3RTcPO"},"source":["\n","!ls -alh /root/.mujoco/\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YVQWcww_uZMo"},"source":["\n","%env LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/root/.mujoco/mujoco200/bin\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AviuDDxpqhOs"},"source":["\n","###### mujoco-py setup ######\n","\n","!pip install mujoco_py==2.0.2.8\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"duUbqfKEordx"},"source":["\n","###### D4RL setup ######\n","\n","## !pip uninstall dm_control==0.0.364896371\n","\n","!git clone https://github.com/rail-berkeley/d4rl.git\n","\n","### edit dm_control version in d4rl setup.py\n","!sed -i \"s;dm_control @ git+git://github.com/deepmind/dm_control@master#egg=dm_control;dm_control==0.0.364896371;g\" /content/d4rl/setup.py\n","\n","### edit mjrl install in d4rl setup.py to use github's new https protocol instead of git SSH\n","!sed -i \"s;mjrl @ git+git://github.com/aravindr93/mjrl@master#egg=mjrl;mjrl @ git+https://github.com/aravindr93/mjrl@master#egg=mjrl;g\" /content/d4rl/setup.py\n","\n","!pip install -e d4rl/.\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jVrmCbNMAwQk"},"source":["\n","###### restart runtime ######\n","\n","exit()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"Piy8RogZccBr"}},{"cell_type":"markdown","source":["# check mujoco-py and D4RL installation\n","\n","* if check fails then **Restart Runtime** again\n","* if check still fails then Factory reset runtime and install again\n","* After installing, first import will be slow as the lib will be built again\n"],"metadata":{"id":"aBD3fRknjEj6"}},{"cell_type":"code","source":["# set mujoco env path if not already set\n","%env LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/root/.mujoco/mujoco200/bin\n","\n","import gym\n","import d4rl # Import required to register environments\n","\n","\n","env = gym.make('Walker2d-v3')\n","env.reset()\n","env.step(env.action_space.sample())\n","env.close()\n","print(\"mujoco-py check passed\")\n","\n","env = gym.make('walker2d-medium-v2')\n","env.reset()\n","env.step(env.action_space.sample())\n","env.close()\n","print(\"d4rl check passed\")\n"],"metadata":{"id":"3uycTGiqjKYK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# download D4RL data\n","\n","*   skip this block if data is already downloaded\n","\n"],"metadata":{"id":"9TpGEYTblzQc"}},{"cell_type":"code","source":["\n","# set mujoco env path if not already set\n","%env LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/root/.mujoco/mujoco200/bin\n","\n","import os\n","import gym\n","import numpy as np\n","\n","import collections\n","import pickle\n","\n","import d4rl\n","\n","datasets = []\n","\n","data_dir = \"./data\"\n","\n","print(data_dir)\n","\n","if not os.path.exists(data_dir):\n","    os.makedirs(data_dir)\n","\n","for env_name in ['walker2d', 'halfcheetah', 'hopper']:\n","    #for dataset_type in ['medium', 'medium-expert', 'medium-replay']:\n","#for env_name in ['walker2d']:\n","     for dataset_type in ['medium']:\n","\n","\n","        name = f'{env_name}-{dataset_type}-v2'\n","        pkl_file_path = os.path.join(data_dir, name)\n","\n","        print(\"processing: \", name)\n","\n","        env = gym.make(name)\n","        dataset = env.get_dataset()\n","\n","        N = dataset['rewards'].shape[0]\n","        data_ = collections.defaultdict(list)\n","\n","        use_timeouts = False\n","        if 'timeouts' in dataset:\n","            use_timeouts = True\n","\n","        episode_step = 0\n","        paths = []\n","        for i in range(N):\n","            done_bool = bool(dataset['terminals'][i])\n","            if use_timeouts:\n","                final_timestep = dataset['timeouts'][i]\n","            else:\n","                final_timestep = (episode_step == 1000-1)\n","            for k in ['observations', 'next_observations', 'actions', 'rewards', 'terminals']:\n","                data_[k].append(dataset[k][i])\n","            if done_bool or final_timestep:\n","                episode_step = 0\n","                episode_data = {}\n","                for k in data_:\n","                    episode_data[k] = np.array(data_[k])\n","                paths.append(episode_data)\n","                data_ = collections.defaultdict(list)\n","            episode_step += 1\n","\n","        returns = np.array([np.sum(p['rewards']) for p in paths])\n","        num_samples = np.sum([p['rewards'].shape[0] for p in paths])\n","        print(f'Number of samples collected: {num_samples}')\n","        print(f'Trajectory returns: mean = {np.mean(returns)}, std = {np.std(returns)}, max = {np.max(returns)}, min = {np.min(returns)}')\n","\n","        with open(f'{pkl_file_path}.pkl', 'wb') as f:\n","            pickle.dump(paths, f)\n","\n"],"metadata":{"id":"V31ELEKOih7D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7a8aVByd9V_S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# import libs\n"],"metadata":{"id":"W2Nk5Gp7hUGA"}},{"cell_type":"code","source":["\n","# set mujoco env path if not already set\n","%env LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/root/.mujoco/mujoco200/bin\n","!pip install product-key-memory\n","import os\n","import sys\n","import random\n","import csv\n","from datetime import datetime\n","import pickle\n","import collections\n","import math\n","\n","import numpy as np\n","import gym\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch.autograd import Variable\n","device = torch.device(\"cuda\")"],"metadata":{"id":"q4xiijmBixUm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!/opt/bin/nvidia-smi"],"metadata":{"id":"O8sBjDyTIZdk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# training parameters"],"metadata":{"id":"QQcLNRgD6SaW"}},{"cell_type":"code","source":["\n","dataset = \"medium\"       # medium / medium-replay / medium-expert\n","rtg_scale = 1000                # scale to normalize returns to go\n","\n","# use v3 env for evaluation because\n","# DT paper evaluates results on v3 envs\n","\n","env_name = 'Walker2d-v3'\n","rtg_target = 5000\n","env_d4rl_name = f'walker2d-{dataset}-v2'\n","\n","#env_name = 'HalfCheetah-v3'\n","#rtg_target = 6000\n","#env_d4rl_name = f'halfcheetah-{dataset}-v2'\n","\n","#env_name = 'Hopper-v3'\n","#rtg_target = 3600\n","#env_d4rl_name = f'hopper-{dataset}-v2'\n","\n","\n","max_eval_ep_len = 1000      # max len of one evaluation episode\n","num_eval_ep = 10            # num of evaluation episodes per iteration\n","\n","batch_size = 64             # training batch size\n","lr = 1e-4                   # learning rate\n","wt_decay = 1e-4             # weight decay\n","warmup_steps = 10000        # warmup steps for lr scheduler\n","\n","# total updates = max_train_iters x num_updates_per_iter\n","max_train_iters = 300\n","num_updates_per_iter = 100\n","\n","context_len = 20        # K in decision transformer\n","n_blocks = 3            # num of transformer blocks\n","embed_dim = 128         # embedding (hidden) dim of transformer\n","n_heads = 1             # num of transformer heads\n","dropout_p = 0.1         # dropout probability\n","\n","\n","\n","# load data from this file\n","dataset_path = f'data/{env_d4rl_name}.pkl'\n","\n","# saves model and csv in this directory\n","log_dir = \"/content/drive/MyDrive/dt_runs/\"\n","\n","\n","if not os.path.exists(log_dir):\n","    os.makedirs(log_dir)\n","\n","\n","# training and evaluation device\n","device_name = 'cuda'\n","device = torch.device(device_name)\n","print(\"device set to: \", device)\n","\n"],"metadata":{"id":"WdtDsvit6m_e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# infos"],"metadata":{"id":"gLHjV3q28LNr"}},{"cell_type":"code","source":["\n","## from infos.py from official d4rl github repo\n","\n","REF_MAX_SCORE = {\n","    'halfcheetah' : 12135.0,\n","    'walker2d' : 4592.3,\n","    'hopper' : 3234.3,\n","}\n","\n","REF_MIN_SCORE = {\n","    'halfcheetah' : -280.178953,\n","    'walker2d' : 1.629008,\n","    'hopper' : -20.272305,\n","}\n","\n","\n","## calculated from d4rl datasets\n","\n","D4RL_DATASET_STATS = {\n","        'halfcheetah-medium-v2': {\n","                'state_mean':[-0.06845773756504059, 0.016414547339081764, -0.18354906141757965, \n","                              -0.2762460708618164, -0.34061527252197266, -0.09339715540409088, \n","                              -0.21321271359920502, -0.0877423882484436, 5.173007488250732, \n","                              -0.04275195300579071, -0.036108363419771194, 0.14053793251514435, \n","                              0.060498327016830444, 0.09550975263118744, 0.06739100068807602, \n","                              0.005627387668937445, 0.013382787816226482\n","                ],\n","                'state_std':[0.07472999393939972, 0.3023499846458435, 0.30207309126853943, \n","                             0.34417077898979187, 0.17619241774082184, 0.507205605506897, \n","                             0.2567007839679718, 0.3294812738895416, 1.2574149370193481, \n","                             0.7600541710853577, 1.9800915718078613, 6.565362453460693, \n","                             7.466367721557617, 4.472222805023193, 10.566964149475098, \n","                             5.671932697296143, 7.4982590675354  \n","                ]\n","            },\n","        'halfcheetah-medium-replay-v2': {\n","                'state_mean':[-0.12880703806877136, 0.3738119602203369, -0.14995987713336945, \n","                              -0.23479078710079193, -0.2841278612613678, -0.13096535205841064, \n","                              -0.20157982409000397, -0.06517726927995682, 3.4768247604370117, \n","                              -0.02785065770149231, -0.015035249292850494, 0.07697279006242752, \n","                              0.01266712136566639, 0.027325302362442017, 0.02316424623131752, \n","                              0.010438721626996994, -0.015839405357837677\n","                ],\n","                'state_std':[0.17019015550613403, 1.284424901008606, 0.33442774415016174, \n","                             0.3672759234905243, 0.26092398166656494, 0.4784106910228729, \n","                             0.3181420564651489, 0.33552637696266174, 2.0931615829467773, \n","                             0.8037433624267578, 1.9044333696365356, 6.573209762573242, \n","                             7.572863578796387, 5.069749355316162, 9.10555362701416, \n","                             6.085654258728027, 7.25300407409668\n","                ]\n","            },\n","        'halfcheetah-medium-expert-v2': {\n","                'state_mean':[-0.05667462572455406, 0.024369969964027405, -0.061670560389757156, \n","                              -0.22351515293121338, -0.2675151228904724, -0.07545716315507889, \n","                              -0.05809682980179787, -0.027675075456500053, 8.110626220703125, \n","                              -0.06136331334710121, -0.17986927926540375, 0.25175222754478455, \n","                              0.24186332523822784, 0.2519369423389435, 0.5879552960395813, \n","                              -0.24090635776519775, -0.030184272676706314\n","                ],\n","                'state_std':[0.06103534251451492, 0.36054104566574097, 0.45544400811195374, \n","                             0.38476887345314026, 0.2218363732099533, 0.5667523741722107, \n","                             0.3196682929992676, 0.2852923572063446, 3.443821907043457, \n","                             0.6728139519691467, 1.8616976737976074, 9.575807571411133, \n","                             10.029894828796387, 5.903450012207031, 12.128185272216797, \n","                             6.4811787605285645, 6.378620147705078\n","                ]\n","            },\n","        'walker2d-medium-v2': {\n","                'state_mean':[1.218966007232666, 0.14163373410701752, -0.03704913705587387, \n","                              -0.13814310729503632, 0.5138224363327026, -0.04719110205769539, \n","                              -0.47288352251052856, 0.042254164814949036, 2.3948874473571777, \n","                              -0.03143199160695076, 0.04466355964541435, -0.023907244205474854, \n","                              -0.1013401448726654, 0.09090937674045563, -0.004192637279629707, \n","                              -0.12120571732521057, -0.5497063994407654\n","                ],\n","                'state_std':[0.12311358004808426, 0.3241879940032959, 0.11456084251403809, \n","                             0.2623065710067749, 0.5640279054641724, 0.2271878570318222, \n","                             0.3837319612503052, 0.7373676896095276, 1.2387926578521729, \n","                             0.798020601272583, 1.5664079189300537, 1.8092705011367798, \n","                             3.025604248046875, 4.062486171722412, 1.4586567878723145, \n","                             3.7445690631866455, 5.5851287841796875\n","                ]\n","            },\n","        'walker2d-medium-replay-v2': {\n","                'state_mean':[1.209364652633667, 0.13264022767543793, -0.14371201395988464, \n","                              -0.2046516090631485, 0.5577612519264221, -0.03231537342071533, \n","                              -0.2784661054611206, 0.19130706787109375, 1.4701707363128662, \n","                              -0.12504704296588898, 0.0564953051507473, -0.09991033375263214, \n","                              -0.340340256690979, 0.03546293452382088, -0.08934258669614792, \n","                              -0.2992438077926636, -0.5984178185462952   \n","                ],\n","                'state_std':[0.11929835379123688, 0.3562574088573456, 0.25852200388908386, \n","                             0.42075422406196594, 0.5202291011810303, 0.15685082972049713, \n","                             0.36770978569984436, 0.7161387801170349, 1.3763766288757324, \n","                             0.8632221817970276, 2.6364643573760986, 3.0134117603302, \n","                             3.720684051513672, 4.867283821105957, 2.6681625843048096, \n","                             3.845186948776245, 5.4768385887146\n","                ]\n","            },\n","        'walker2d-medium-expert-v2': {\n","                'state_mean':[1.2294334173202515, 0.16869689524173737, -0.07089081406593323, \n","                              -0.16197483241558075, 0.37101927399635315, -0.012209027074277401, \n","                              -0.42461398243904114, 0.18986578285694122, 3.162475109100342, \n","                              -0.018092676997184753, 0.03496946766972542, -0.013921679928898811, \n","                              -0.05937029421329498, -0.19549426436424255, -0.0019200450042262673, \n","                              -0.062483321875333786, -0.27366524934768677\n","                ],\n","                'state_std':[0.09932824969291687, 0.25981399416923523, 0.15062759816646576, \n","                             0.24249176681041718, 0.6758718490600586, 0.1650741547346115, \n","                             0.38140663504600525, 0.6962361335754395, 1.3501490354537964, \n","                             0.7641991376876831, 1.534574270248413, 2.1785972118377686, \n","                             3.276582717895508, 4.766193866729736, 1.1716983318328857, \n","                             4.039782524108887, 5.891613960266113       \n","                ]\n","            },\n","        'hopper-medium-v2': {\n","                'state_mean':[1.311279058456421, -0.08469521254301071, -0.5382719039916992, \n","                              -0.07201576232910156, 0.04932365566492081, 2.1066856384277344, \n","                              -0.15017354488372803, 0.008783451281487942, -0.2848185896873474, \n","                              -0.18540096282958984, -0.28461286425590515\n","                ],\n","                'state_std':[0.17790751159191132, 0.05444620922207832, 0.21297138929367065, \n","                             0.14530418813228607, 0.6124444007873535, 0.8517446517944336, \n","                             1.4515252113342285, 0.6751695871353149, 1.5362390279769897, \n","                             1.616074562072754, 5.607253551483154 \n","                ]\n","            },\n","        'hopper-medium-replay-v2': {\n","                'state_mean':[1.2305138111114502, -0.04371410980820656, -0.44542956352233887, \n","                              -0.09370097517967224, 0.09094487875699997, 1.3694725036621094, \n","                              -0.19992674887180328, -0.022861352190375328, -0.5287045240402222, \n","                              -0.14465883374214172, -0.19652697443962097      \n","                ],\n","                'state_std':[0.1756512075662613, 0.0636928603053093, 0.3438323438167572, \n","                             0.19566889107227325, 0.5547984838485718, 1.051029920578003, \n","                             1.158307671546936, 0.7963128685951233, 1.4802359342575073, \n","                             1.6540331840515137, 5.108601093292236\n","                ]\n","            },\n","        'hopper-medium-expert-v2': {\n","                'state_mean':[1.3293815851211548, -0.09836531430482864, -0.5444297790527344, \n","                              -0.10201650857925415, 0.02277466468513012, 2.3577215671539307, \n","                              -0.06349576264619827, -0.00374026270583272, -0.1766270101070404, \n","                              -0.11862941086292267, -0.12097819894552231\n","                ],\n","                'state_std':[0.17012375593185425, 0.05159067362546921, 0.18141433596611023, \n","                             0.16430604457855225, 0.6023368239402771, 0.7737284898757935, \n","                             1.4986555576324463, 0.7483318448066711, 1.7953159809112549, \n","                             2.0530025959014893, 5.725032806396484\n","                ]\n","            },\n","    }\n","\n"],"metadata":{"id":"btnq_IL_j4PO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# utils"],"metadata":{"id":"pewE01Ca4BG0"}},{"cell_type":"code","source":["def evaluate_episode(\n","        env,\n","        state_dim,\n","        act_dim,\n","        model,\n","        max_ep_len=1000,\n","        device='cuda',\n","        target_return=None,\n","        mode='normal',\n","        state_mean=0.,\n","        state_std=1.,\n","):\n","\n","    model.eval()\n","    model.to(device=device)\n","\n","    state_mean = torch.from_numpy(state_mean).to(device=device)\n","    state_std = torch.from_numpy(state_std).to(device=device)\n","\n","    state = env.reset()\n","\n","    # we keep all the histories on the device\n","    # note that the latest action and reward will be \"padding\"\n","    states = torch.from_numpy(state).reshape(1, state_dim).to(device=device, dtype=torch.float32)\n","    actions = torch.zeros((0, act_dim), device=device, dtype=torch.float32)\n","    rewards = torch.zeros(0, device=device, dtype=torch.float32)\n","    target_return = torch.tensor(target_return, device=device, dtype=torch.float32)\n","    sim_states = []\n","\n","    episode_return, episode_length = 0, 0\n","    for t in range(max_ep_len):\n","\n","        # add padding\n","        actions = torch.cat([actions, torch.zeros((1, act_dim), device=device)], dim=0)\n","        rewards = torch.cat([rewards, torch.zeros(1, device=device)])\n","\n","        action = model.get_action(\n","            (states.to(dtype=torch.float32) - state_mean) / state_std,\n","            actions.to(dtype=torch.float32),\n","            rewards.to(dtype=torch.float32),\n","            target_return=target_return,\n","        )\n","        actions[-1] = action\n","        action = action.detach().cpu().numpy()\n","\n","        state, reward, done, _ = env.step(action)\n","\n","        cur_state = torch.from_numpy(state).to(device=device).reshape(1, state_dim)\n","        states = torch.cat([states, cur_state], dim=0)\n","        rewards[-1] = reward\n","\n","        episode_return += reward\n","        episode_length += 1\n","\n","        if done:\n","            break\n","\n","    return episode_return, episode_length"],"metadata":{"id":"mlwOPxM3ylAc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","def discount_cumsum(x, gamma):\n","    disc_cumsum = np.zeros_like(x)\n","    disc_cumsum[-1] = x[-1]\n","    for t in reversed(range(x.shape[0]-1)):\n","        disc_cumsum[t] = x[t] + gamma * disc_cumsum[t+1]\n","    return disc_cumsum\n","\n","\n","def get_d4rl_dataset_stats(env_d4rl_name):\n","    return D4RL_DATASET_STATS[env_d4rl_name]\n","\n","\n","def get_d4rl_normalized_score(score, env_name):\n","    env_key = env_name.split('-')[0].lower()\n","    assert env_key in REF_MAX_SCORE, f'no reference score for {env_key} env to calculate d4rl score'\n","    return (score - REF_MIN_SCORE[env_key]) / (REF_MAX_SCORE[env_key] - REF_MIN_SCORE[env_key])\n","    \n","    \n","def evaluate_on_env(model, device, context_len, env, rtg_target, rtg_scale,\n","                    num_eval_ep=10, max_test_ep_len=1000,\n","                    state_mean=None, state_std=None, render=False):\n","\n","    eval_batch_size = 1  # required for forward pass\n","\n","    results = {}\n","    total_reward = 0\n","    total_timesteps = 0\n","\n","    state_dim = env.observation_space.shape[0]\n","    act_dim = env.action_space.shape[0]\n","\n","    if state_mean is None:\n","        state_mean = torch.zeros((state_dim,)).to(device)\n","    else:\n","        state_mean = torch.from_numpy(state_mean).to(device)\n","        \n","    if state_std is None:\n","        state_std = torch.ones((state_dim,)).to(device)\n","    else:\n","        state_std = torch.from_numpy(state_std).to(device)\n","\n","    # same as timesteps used for training the transformer\n","    # also, crashes if device is passed to arange()\n","    timesteps = torch.arange(start=0, end=max_test_ep_len, step=1)\n","    timesteps = timesteps.repeat(eval_batch_size, 1).to(device)\n","\n","    model.eval()\n","\n","    with torch.no_grad():\n","\n","        for _ in range(num_eval_ep):\n","\n","            # zeros place holders\n","            actions = torch.zeros((eval_batch_size, max_test_ep_len, act_dim),\n","                                dtype=torch.float32, device=device)\n","\n","            states = torch.zeros((eval_batch_size, max_test_ep_len, state_dim),\n","                                dtype=torch.float32, device=device)\n","            \n","            rewards_to_go = torch.zeros((eval_batch_size, max_test_ep_len, 1),\n","                                dtype=torch.float32, device=device)\n","            \n","            # init episode\n","            running_state = env.reset()\n","            running_reward = 0\n","            running_rtg = rtg_target / rtg_scale\n","\n","            for t in range(max_test_ep_len):\n","\n","                total_timesteps += 1\n","\n","                # add state in placeholder and normalize\n","                states[0, t] = torch.from_numpy(running_state).to(device)\n","                states[0, t] = (states[0, t] - state_mean) / state_std\n","\n","                # calcualate running rtg and add in placeholder\n","                running_rtg = running_rtg - (running_reward / rtg_scale)\n","                rewards_to_go[0, t] = running_rtg\n","\n","                if t < context_len:\n","                    _, act_preds, _ = model.forward(timesteps[:,:context_len],\n","                                                states[:,:context_len],\n","                                                actions[:,:context_len],\n","                                                rewards_to_go[:,:context_len])\n","                    act = act_preds[0, t].detach()\n","                else:\n","                    _, act_preds, _ = model.forward(timesteps[:,t-context_len+1:t+1],\n","                                                states[:,t-context_len+1:t+1],\n","                                                actions[:,t-context_len+1:t+1],\n","                                                rewards_to_go[:,t-context_len+1:t+1])\n","                    act = act_preds[0, -1].detach()\n","\n","\n","                running_state, running_reward, done, _ = env.step(act.cpu().numpy())\n","\n","                # add action in placeholder\n","                actions[0, t] = act\n","\n","                total_reward += running_reward\n","\n","                if render:\n","                    env.render()\n","                if done:\n","                    break\n","\n","    results['eval/avg_reward'] = total_reward / num_eval_ep\n","    results['eval/avg_ep_len'] = total_timesteps / num_eval_ep\n","    \n","    return results\n","\n"],"metadata":{"id":"EaaymCHPlynF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# dataset"],"metadata":{"id":"QXXrs_PjAHrN"}},{"cell_type":"code","source":["## check data\n","\n","# load dataset\n","with open(dataset_path, 'rb') as f:\n","    trajectories = pickle.load(f)\n","\n","min_len = 10**4\n","states = []\n","for traj in trajectories:\n","    min_len = min(min_len, traj['observations'].shape[0])\n","    states.append(traj['observations'])\n","\n","# used for input normalization\n","states = np.concatenate(states, axis=0)\n","state_mean, state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n","\n","print(dataset_path)\n","print(\"num of trajectories in dataset: \", len(trajectories))\n","print(\"minimum trajectory length in dataset: \", min_len)\n","print(\"state mean: \", state_mean.tolist())\n","print(\"state std: \", state_std.tolist())\n","\n","\n","## check if info is correct\n","print(\"is state mean info correct: \", state_mean.tolist() == D4RL_DATASET_STATS[env_d4rl_name]['state_mean'])\n","print(\"is state std info correct: \", state_std.tolist() == D4RL_DATASET_STATS[env_d4rl_name]['state_std'])\n","\n","\n","assert state_mean.tolist() == D4RL_DATASET_STATS[env_d4rl_name]['state_mean']\n","assert state_std.tolist() == D4RL_DATASET_STATS[env_d4rl_name]['state_std']\n"],"metadata":{"id":"n1Vb5rY_iiME"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class D4RLTrajectoryDataset(Dataset):\n","    def __init__(self, dataset_path, context_len, rtg_scale):\n","\n","        self.context_len = context_len        \n","        \n","        # Action Error Test \n","        self.ErrorP = 0.05\n","\n","        # load dataset\n","        with open(dataset_path, 'rb') as f:\n","            self.trajectories = pickle.load(f)\n","        \n","        # calculate min len of traj, state mean and variance\n","        # and returns_to_go for all traj\n","        min_len = 10**6\n","        states = []\n","        for traj in self.trajectories:\n","            traj_len = traj['observations'].shape[0]\n","            min_len = min(min_len, traj_len)\n","            states.append(traj['observations'])\n","            # calculate returns to go and rescale them\n","            traj['returns_to_go'] = discount_cumsum(traj['rewards'], 1.0) / rtg_scale\n","\n","        # used for input normalization\n","        states = np.concatenate(states, axis=0)\n","        self.state_mean, self.state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n","\n","        # normalize states\n","        for traj in self.trajectories:\n","            traj['observations'] = (traj['observations'] - self.state_mean) / self.state_std\n","\n","\n","    def get_state_stats(self):\n","        return self.state_mean, self.state_std\n","\n","    def __len__(self):\n","        return len(self.trajectories)\n","\n","    def __getitem__(self, idx):\n","        traj = self.trajectories[idx] \n","        traj_len = traj['observations'].shape[0]\n","\n","        if traj_len >= self.context_len:\n","            # sample random index to slice trajectory\n","            si = random.randint(0, traj_len - self.context_len)\n","\n","            states = torch.from_numpy(traj['observations'][si : si + self.context_len])\n","            actions = torch.from_numpy(traj['actions'][si : si + self.context_len])\n","            returns_to_go = torch.from_numpy(traj['returns_to_go'][si : si + self.context_len])\n","            timesteps = torch.arange(start=si, end=si+self.context_len, step=1)\n","\n","            if self.ErrorP > 0:\n","              # Introduce errors with probability ErrorP\n","              mask = torch.rand(actions.shape) < self.ErrorP\n","              mask = mask.to(torch.bool)\n","              error_values = np.random.uniform(-1, 1, actions.shape)  # Modify the range of error values as needed\n","              error_values = torch.from_numpy(error_values).to(actions.dtype)\n","\n","              # Apply errors to actions\n","              actions = torch.where(mask, actions + error_values, actions)\n","              \n","            # all ones since no padding\n","            traj_mask = torch.ones(self.context_len, dtype=torch.long)\n","\n","        else:\n","            padding_len = self.context_len - traj_len\n","\n","            # padding with zeros\n","            states = torch.from_numpy(traj['observations'])\n","            states = torch.cat([states,\n","                                torch.zeros(([padding_len] + list(states.shape[1:])),\n","                                dtype=states.dtype)], \n","                               dim=0)\n","            \n","            actions = torch.from_numpy(traj['actions'])\n","            actions = torch.cat([actions,\n","                                torch.zeros(([padding_len] + list(actions.shape[1:])),\n","                                dtype=actions.dtype)], \n","                               dim=0)\n","\n","            returns_to_go = torch.from_numpy(traj['returns_to_go'])\n","            returns_to_go = torch.cat([returns_to_go,\n","                                torch.zeros(([padding_len] + list(returns_to_go.shape[1:])),\n","                                dtype=returns_to_go.dtype)], \n","                               dim=0)\n","            \n","            timesteps = torch.arange(start=0, end=self.context_len, step=1)\n","\n","            traj_mask = torch.cat([torch.ones(traj_len, dtype=torch.long), \n","                                   torch.zeros(padding_len, dtype=torch.long)], \n","                                  dim=0)\n","            \n","        return  timesteps, states, actions, returns_to_go, traj_mask\n","\n","\n"],"metadata":{"id":"eo4zPTjjn0Qr","executionInfo":{"status":"ok","timestamp":1682380153404,"user_tz":240,"elapsed":143,"user":{"displayName":"Daniel Z Qiu","userId":"05550563715393118980"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d3d6e3c0-2a86-49cf-d0e5-356a4222250d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"code","source":["class FeedForward(nn.Module):\n","    def __init__(self, dim, hidden_dim, dropout = 0.):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_dim, dim),\n","            nn.Dropout(dropout)\n","        )\n","    def forward(self, x):\n","        return self.net(x)\n","      \n","class PreNorm(nn.Module):\n","    def __init__(self, dim, fn):\n","        super().__init__()\n","        self.norm = nn.LayerNorm(dim)\n","        self.fn = fn\n","    def forward(self, x, **kwargs):\n","        return self.fn(self.norm(x), **kwargs)\n","\n","class FNetBlock(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","  def forward(self, x):\n","    x = torch.fft.fft(torch.fft.fft(x, dim=-1), dim=-2).real\n","    return x\n","\n","class FNet(nn.Module):\n","    def __init__(self, dim, depth, mlp_dim, dropout = 0.1):\n","        super().__init__()\n","        self.layers = nn.ModuleList([])\n","        for _ in range(depth):\n","            self.layers.append(nn.ModuleList([\n","                PreNorm(dim, FNetBlock()),\n","                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n","            ]))\n","    def forward(self, x):\n","        for attn, ff in self.layers:\n","            x = attn(x) + x\n","            x = ff(x) + x\n","        return x"],"metadata":{"id":"ESZ4Ka1dTWgM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Block(nn.Module):\n","    def __init__(self, h_dim, max_T, n_heads, drop_p):\n","        super().__init__()\n","        # self.attention = MaskedCausalAttention(h_dim, max_T, n_heads, drop_p)\n","       \n","        \"\"\"\n","        self.attention = AutoCorrelationLayer(\n","            AutoCorrelation(True, attention_dropout=drop_p,\n","                            output_attention=False),\n","            h_dim, n_heads)\n","        \"\"\"\n","        self.attention = FNetBlock()\n","        self.mlp = nn.Sequential(\n","                nn.Linear(h_dim, 4*h_dim),\n","                nn.GELU(),\n","                nn.Linear(4*h_dim, h_dim),\n","                nn.Dropout(drop_p),\n","            )\n","        self.ln1 = nn.LayerNorm(h_dim)\n","        self.ln2 = nn.LayerNorm(h_dim)\n","\n","    def forward(self, x):\n","        # Attention -> LayerNorm -> MLP -> LayerNorm\n","        x = x + self.attention(x) # residual\n","        x = self.ln1(x)\n","        x = x + self.mlp(x) # residual\n","        x = self.ln2(x)\n","        return x\n","\n","class Decoder(nn.Module):\n","      def __init__(self, h_dim, max_T, n_heads, drop_p):\n","          super().__init__()\n","          self.h_dim = h_dim\n","          self.max_T = max_T\n","          self.n_heads = n_heads\n","          self.drop_p = drop_p\n","\n","      def forward(self, x):\n","          blocks = [Block(self.h_dim, self.max_T, self.n_heads, self.drop_p) for _ in range(n_blocks)]\n","          output = nn.Sequential(*blocks)\n","          return output"],"metadata":{"id":"XSzYVPr2TLuy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class DecisionFFT(nn.Module):\n","    def __init__(self, state_dim, act_dim, n_blocks, h_dim, context_len, \n","                 n_heads, drop_p, max_timestep=4096):\n","        super().__init__()\n","\n","        self.state_dim = state_dim\n","        self.act_dim = act_dim\n","        self.h_dim = h_dim\n","\n","        ### transformer blocks\n","        input_seq_len = 3 * context_len\n","        #blocks = [Block(h_dim, input_seq_len, n_heads, drop_p) for _ in range(n_blocks)]\n","        self.transformer = FNet(dim=h_dim, depth=n_blocks, mlp_dim=h_dim*4)\n","        \n","        #self.projection = nn.Linear(h_dim, input_seq_len, bias=True)\n","\n","        ### projection heads (project to embedding)\n","        self.embed_ln = nn.LayerNorm(h_dim)\n","        self.embed_timestep = nn.Embedding(max_timestep, h_dim)\n","        self.embed_rtg = torch.nn.Linear(1, h_dim)\n","        self.embed_state = torch.nn.Linear(state_dim, h_dim)\n","        \n","        # # discrete actions\n","        # self.embed_action = torch.nn.Embedding(act_dim, h_dim)\n","        # use_action_tanh = False # False for discrete actions\n","\n","        # continuous actions\n","        self.embed_action = torch.nn.Linear(act_dim, h_dim)\n","        use_action_tanh = True # True for continuous actions\n","        \n","        ### prediction heads\n","        self.predict_rtg = torch.nn.Linear(h_dim, 1)\n","        self.predict_state = torch.nn.Linear(h_dim, state_dim)\n","        self.predict_action = nn.Sequential(\n","            *([nn.Linear(h_dim, act_dim)] + ([nn.Tanh()] if use_action_tanh else []))\n","        )\n","\n","\n","    def forward(self, timesteps, states, actions, returns_to_go):\n","\n","        B, T, _ = states.shape\n","\n","        time_embeddings = self.embed_timestep(timesteps)\n","\n","        # time embeddings are treated similar to positional embeddings\n","        state_embeddings = self.embed_state(states) + time_embeddings\n","        action_embeddings = self.embed_action(actions) + time_embeddings\n","        returns_embeddings = self.embed_rtg(returns_to_go) + time_embeddings\n","\n","        # stack rtg, states and actions and reshape sequence as\n","        # (r1, s1, a1, r2, s2, a2 ...)\n","        h = torch.stack(\n","            (returns_embeddings, state_embeddings, action_embeddings), dim=1\n","        ).permute(0, 2, 1, 3).reshape(B, 3 * T, self.h_dim)\n","\n","        h = self.embed_ln(h)\n","        \n","        # transformer and prediction\n","        h = self.transformer(h)\n","        #h = self.projection(h)\n","\n","        # get h reshaped such that its size = (B x 3 x T x h_dim) and\n","        # h[:, 0, t] is conditioned on r_0, s_0, a_0 ... r_t\n","        # h[:, 1, t] is conditioned on r_0, s_0, a_0 ... r_t, s_t\n","        # h[:, 2, t] is conditioned on r_0, s_0, a_0 ... r_t, s_t, a_t\n","        h = h.reshape(B, T, 3, self.h_dim).permute(0, 2, 1, 3)\n","\n","        # get predictions\n","        return_preds = self.predict_rtg(h[:,2])     # predict next rtg given r, s, a\n","        state_preds = self.predict_state(h[:,2])    # predict next state given r, s, a\n","        action_preds = self.predict_action(h[:,1])  # predict action given r, s\n","    \n","        return state_preds, action_preds, return_preds\n"],"metadata":{"id":"atzUr4kiTs1p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Informer"],"metadata":{"id":"HLns9_iJvgA3"}},{"cell_type":"code","source":["\n","class DecoderLayer(nn.Module):\n","    def __init__(self, self_attention,  d_model, d_ff=None,cross_attention = None,\n","                 dropout=0.1, activation=\"relu\"):\n","        super(DecoderLayer, self).__init__()\n","        d_ff = d_ff or 4*d_model\n","        self.self_attention = self_attention\n","        #self.cross_attention = cross_attention\n","        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n","        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        #self.norm3 = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","        self.activation = F.relu if activation == \"relu\" else F.gelu\n","\n","    def forward(self, x,  x_mask=None, cross_mask=None):\n","        \n","        x = x + self.dropout(self.self_attention(\n","            x, x, x,\n","            attn_mask=x_mask\n","        )[0])\n","        \n","        #x = x + self.self_attention(x, x, x,attn_mask=x_mask)\n","        \n","        y = x = self.norm1(x)\n","\n","        \n","        y = self.dropout(self.activation(self.conv1(y.transpose(-1,1))))\n","        y = self.dropout(self.conv2(y).transpose(-1,1))\n","\n","        return self.norm2(x+y)\n","\n","class Decoder(nn.Module):\n","    def __init__(self, layers, norm_layer=None):\n","        super(Decoder, self).__init__()\n","        self.layers = nn.ModuleList(layers)\n","        self.norm = norm_layer\n","\n","    def forward(self, x,  x_mask=None, cross_mask=None):\n","        for layer in self.layers:\n","            x = layer(x, x_mask=x_mask, cross_mask=cross_mask)\n","\n","        if self.norm is not None:\n","            x = self.norm(x)\n","\n","        return x"],"metadata":{"id":"meBXMKtawHpY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682374532571,"user_tz":240,"elapsed":138,"user":{"displayName":"Daniel Z Qiu","userId":"05550563715393118980"}},"outputId":"bf873f9c-06fd-4193-d8ba-734a0bcbe4be"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"code","source":["from math import sqrt\n","class ProbMask():\n","    def __init__(self, B, H, L, index, scores, device=\"cpu\"):\n","        _mask = torch.ones(L, scores.shape[-1], dtype=torch.bool).to(device).triu(1)\n","        _mask_ex = _mask[None, None, :].expand(B, H, L, scores.shape[-1])\n","        indicator = _mask_ex[torch.arange(B)[:, None, None],\n","                             torch.arange(H)[None, :, None],\n","                             index, :].to(device)\n","        self._mask = indicator.view(scores.shape).to(device)\n","    \n","    @property\n","    def mask(self):\n","        return self._mask\n","\n","class ProbAttention(nn.Module):\n","    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n","        super(ProbAttention, self).__init__()\n","        self.factor = factor\n","        self.scale = scale\n","        self.mask_flag = mask_flag\n","        self.output_attention = output_attention\n","        self.dropout = nn.Dropout(attention_dropout)\n","\n","    def _prob_QK(self, Q, K, sample_k, n_top): # n_top: c*ln(L_q)\n","        # Q [B, H, L, D]\n","        B, H, L_K, E = K.shape\n","        _, _, L_Q, _ = Q.shape\n","\n","        # calculate the sampled Q_K\n","        K_expand = K.unsqueeze(-3).expand(B, H, L_Q, L_K, E)\n","        index_sample = torch.randint(L_K, (L_Q, sample_k)) # real U = U_part(factor*ln(L_k))*L_q\n","        K_sample = K_expand[:, :, torch.arange(L_Q).unsqueeze(1), index_sample, :]\n","        Q_K_sample = torch.matmul(Q.unsqueeze(-2), K_sample.transpose(-2, -1)).squeeze(-2)\n","\n","        # find the Top_k query with sparisty measurement\n","        M = Q_K_sample.max(-1)[0] - torch.div(Q_K_sample.sum(-1), L_K)\n","        M_top = M.topk(n_top, sorted=False)[1]\n","\n","        # use the reduced Q to calculate Q_K\n","        Q_reduce = Q[torch.arange(B)[:, None, None],\n","                     torch.arange(H)[None, :, None],\n","                     M_top, :] # factor*ln(L_q)\n","        Q_K = torch.matmul(Q_reduce, K.transpose(-2, -1)) # factor*ln(L_q)*L_k\n","\n","        return Q_K, M_top\n","\n","    def _get_initial_context(self, V, L_Q):\n","        B, H, L_V, D = V.shape\n","        if not self.mask_flag:\n","            # V_sum = V.sum(dim=-2)\n","            V_sum = V.mean(dim=-2)\n","            contex = V_sum.unsqueeze(-2).expand(B, H, L_Q, V_sum.shape[-1]).clone()\n","        else: # use mask\n","            assert(L_Q == L_V) # requires that L_Q == L_V, i.e. for self-attention only\n","            contex = V.cumsum(dim=-2)\n","        return contex\n","\n","    def _update_context(self, context_in, V, scores, index, L_Q, attn_mask):\n","        B, H, L_V, D = V.shape\n","\n","        if self.mask_flag:\n","            attn_mask = ProbMask(B, H, L_Q, index, scores, device=V.device)\n","            scores.masked_fill_(attn_mask.mask, -np.inf)\n","\n","        attn = torch.softmax(scores, dim=-1) # nn.Softmax(dim=-1)(scores)\n","\n","        context_in[torch.arange(B)[:, None, None],\n","                   torch.arange(H)[None, :, None],\n","                   index, :] = torch.matmul(attn, V).type_as(context_in)\n","        if self.output_attention:\n","            attns = (torch.ones([B, H, L_V, L_V])/L_V).type_as(attn).to(attn.device)\n","            attns[torch.arange(B)[:, None, None], torch.arange(H)[None, :, None], index, :] = attn\n","            return (context_in, attns)\n","        else:\n","            return (context_in, None)\n","\n","    def forward(self, queries, keys, values, attn_mask):\n","        B, L_Q, H, D = queries.shape\n","        _, L_K, _, _ = keys.shape\n","\n","        queries = queries.transpose(2,1)\n","        keys = keys.transpose(2,1)\n","        values = values.transpose(2,1)\n","\n","        U_part = self.factor * np.ceil(np.log(L_K)).astype('int').item() # c*ln(L_k)\n","        u = self.factor * np.ceil(np.log(L_Q)).astype('int').item() # c*ln(L_q) \n","\n","        U_part = U_part if U_part<L_K else L_K\n","        u = u if u<L_Q else L_Q\n","        \n","        scores_top, index = self._prob_QK(queries, keys, sample_k=U_part, n_top=u) \n","\n","        # add scale factor\n","        scale = self.scale or 1./sqrt(D)\n","        if scale is not None:\n","            scores_top = scores_top * scale\n","        # get the context\n","        context = self._get_initial_context(values, L_Q)\n","        # update the context with selected top_k queries\n","        context, attn = self._update_context(context, values, scores_top, index, L_Q, attn_mask)\n","        \n","        return context.transpose(2,1).contiguous(), attn\n","\n","\n","class AttentionLayer(nn.Module):\n","    def __init__(self, attention = \"ProbAttention\", d_model = 128, n_heads = 1, \n","                 d_keys=None, d_values=None, mix=False):\n","        super(AttentionLayer, self).__init__()\n","\n","        d_keys = d_keys or (d_model//n_heads)\n","        d_values = d_values or (d_model//n_heads)\n","\n","        self.inner_attention = attention\n","        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n","        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n","        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n","        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n","        self.n_heads = n_heads\n","        self.mix = mix\n","\n","    def forward(self, queries, keys, values, attn_mask):\n","        B, L, _ = queries.shape\n","        _, S, _ = keys.shape\n","        H = self.n_heads\n","\n","        queries = self.query_projection(queries).view(B, L, H, -1)\n","        keys = self.key_projection(keys).view(B, S, H, -1)\n","        values = self.value_projection(values).view(B, S, H, -1)\n","\n","        out, attn = self.inner_attention(\n","            queries,\n","            keys,\n","            values,\n","            attn_mask\n","        )\n","        if self.mix:\n","            out = out.transpose(2,1).contiguous()\n","        out = out.view(B, L, -1)\n","\n","        return self.out_projection(out)#, attn"],"metadata":{"id":"FGTf7oS0vhD2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class MaskedCausalAttention(nn.Module):\n","    def __init__(self, h_dim, max_T, n_heads, drop_p):\n","        super().__init__()\n","\n","        self.n_heads = n_heads\n","        self.max_T = max_T\n","\n","        self.q_net = nn.Linear(h_dim, h_dim)\n","        self.k_net = nn.Linear(h_dim, h_dim)\n","        self.v_net = nn.Linear(h_dim, h_dim)\n","\n","        self.proj_net = nn.Linear(h_dim, h_dim)\n","\n","        self.att_drop = nn.Dropout(drop_p)\n","        self.proj_drop = nn.Dropout(drop_p)\n","\n","        ones = torch.ones((max_T, max_T))\n","        mask = torch.tril(ones).view(1, 1, max_T, max_T)\n","\n","        # register buffer makes sure mask does not get updated\n","        # during backpropagation\n","        self.register_buffer('mask',mask)\n","\n","    def forward(self, x):\n","        B, T, C = x.shape # batch size, seq length, h_dim * n_heads\n","\n","        N, D = self.n_heads, C // self.n_heads # N = num heads, D = attention dim\n","\n","        # rearrange q, k, v as (B, N, T, D)\n","        q = self.q_net(x).view(B, T, N, D).transpose(1,2)\n","        k = self.k_net(x).view(B, T, N, D).transpose(1,2)\n","        v = self.v_net(x).view(B, T, N, D).transpose(1,2)\n","\n","        # weights (B, N, T, T)\n","        weights = q @ k.transpose(2,3) / math.sqrt(D)\n","        # causal mask applied to weights\n","        weights = weights.masked_fill(self.mask[...,:T,:T] == 0, float('-inf'))\n","        # normalize weights, all -inf -> 0 after softmax\n","        normalized_weights = F.softmax(weights, dim=-1)\n","\n","        # attention (B, N, T, D)\n","        attention = self.att_drop(normalized_weights @ v)\n","\n","        # gather heads and project (B, N, T, D) -> (B, T, N*D)\n","        attention = attention.transpose(1, 2).contiguous().view(B,T,N*D)\n","\n","        out = self.proj_drop(self.proj_net(attention))\n","        return out\n","\n","\n","class Block(nn.Module):\n","    def __init__(self, h_dim, max_T, n_heads, drop_p):\n","        super().__init__()\n","        \n","        self.attention = AttentionLayer(d_model = h_dim, n_heads = n_heads)\n","        self.mlp = nn.Sequential(\n","                nn.Linear(h_dim, 4*h_dim),\n","                nn.GELU(),\n","                nn.Linear(4*h_dim, h_dim),\n","                nn.Dropout(drop_p),\n","            )\n","        self.ln1 = nn.LayerNorm(h_dim)\n","        self.ln2 = nn.LayerNorm(h_dim)\n","\"\"\"\n","    def forward(self, x, x_mask=None):\n","        # Attention -> LayerNorm -> MLP -> LayerNorm\n","        x = x + self.attention(x,x,x,x_mask) # residual\n","        x = self.ln1(x)\n","        x = x + self.mlp(x) # residual\n","        x = self.ln2(x)\n","        return x\n","\"\"\"\n","\n","\n","\n","\n","\n","class DecisionInformer(nn.Module):\n","    def __init__(self, state_dim, act_dim, n_blocks, h_dim, context_len, \n","                 n_heads, drop_p, max_timestep=4096):\n","        super().__init__()\n","        \n","        self.state_dim = state_dim\n","        self.act_dim = act_dim\n","        self.h_dim = h_dim\n","        input_seq_len = 3 * context_len\n","        \n","        \n","        self.transformer = Decoder(\n","              [\n","                  DecoderLayer(\n","                      AttentionLayer(ProbAttention(True, factor=5, attention_dropout=drop_p, output_attention=False), \n","                                  h_dim, n_heads, mix=False),\n","                      \n","                      d_model = h_dim,\n","                      d_ff = h_dim*4,\n","                      dropout = drop_p,\n","                      activation=\"gelu\",\n","                  )\n","                  for l in range(n_blocks)\n","              ],\n","              norm_layer=torch.nn.LayerNorm(h_dim)\n","        )\n","        \n","        ### projection heads (project to embedding)\n","        self.embed_ln = nn.LayerNorm(h_dim)\n","        self.embed_timestep = nn.Embedding(max_timestep, h_dim)\n","        self.embed_rtg = torch.nn.Linear(1, h_dim)\n","        self.embed_state = torch.nn.Linear(state_dim, h_dim)\n","        \n","        # # discrete actions\n","        # self.embed_action = torch.nn.Embedding(act_dim, h_dim)\n","        # use_action_tanh = False # False for discrete actions\n","\n","        # continuous actions\n","        self.embed_action = torch.nn.Linear(act_dim, h_dim)\n","        use_action_tanh = True # True for continuous actions\n","        \n","        ### prediction heads\n","        self.predict_rtg = torch.nn.Linear(h_dim, 1)\n","        self.predict_state = torch.nn.Linear(h_dim, state_dim)\n","        self.predict_action = nn.Sequential(\n","            *([nn.Linear(h_dim, act_dim)] + ([nn.Tanh()] if use_action_tanh else []))\n","        )\n","\n","\n","    def forward(self, timesteps, states, actions, returns_to_go):\n","\n","        B, T, _ = states.shape\n","\n","        time_embeddings = self.embed_timestep(timesteps)\n","\n","        # time embeddings are treated similar to positional embeddings\n","        state_embeddings = self.embed_state(states) + time_embeddings\n","        action_embeddings = self.embed_action(actions) + time_embeddings\n","        returns_embeddings = self.embed_rtg(returns_to_go) + time_embeddings\n","\n","        # stack rtg, states and actions and reshape sequence as\n","        # (r1, s1, a1, r2, s2, a2 ...)\n","        h = torch.stack(\n","            (returns_embeddings, state_embeddings, action_embeddings), dim=1\n","        ).permute(0, 2, 1, 3).reshape(B, 3 * T, self.h_dim)\n","\n","        h = self.embed_ln(h)\n","        \n","        # transformer and prediction\n","        h = self.transformer(h)\n","\n","        # get h reshaped such that its size = (B x 3 x T x h_dim) and\n","        # h[:, 0, t] is conditioned on r_0, s_0, a_0 ... r_t\n","        # h[:, 1, t] is conditioned on r_0, s_0, a_0 ... r_t, s_t\n","        # h[:, 2, t] is conditioned on r_0, s_0, a_0 ... r_t, s_t, a_t\n","        h = h.reshape(B, T, 3, self.h_dim).permute(0, 2, 1, 3)\n","\n","        # get predictions\n","        return_preds = self.predict_rtg(h[:,2])     # predict next rtg given r, s, a\n","        state_preds = self.predict_state(h[:,2])    # predict next state given r, s, a\n","        action_preds = self.predict_action(h[:,1])  # predict action given r, s\n","    \n","        return state_preds, action_preds, return_preds"],"metadata":{"id":"cizZthajwIcS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Reformer"],"metadata":{"id":"gYAkkmYGC4OG"}},{"cell_type":"code","source":["from functools import partial\n","!pip install einops\n","!pip install local-attention\n","\n","def top_p(logits, thres = 0.9):\n","    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n","    cum_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n","\n","    sorted_indices_to_remove = cum_probs > (1 - thres)\n","    sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n","    sorted_indices_to_remove[:, 0] = 0\n","\n","    sorted_logits[sorted_indices_to_remove] = float('-inf')\n","    return sorted_logits.scatter(1, sorted_indices, sorted_logits)\n","\n","def top_k(logits, thres = 0.9):\n","    k = int((1 - thres) * logits.shape[-1])\n","    val, ind = torch.topk(logits, k)\n","    probs = torch.full_like(logits, float('-inf'))\n","    probs.scatter_(1, ind, val)\n","    return probs\n","\n","class TrainingWrapper(nn.Module):\n","    def __init__(self, net, ignore_index = -100, pad_value = 0):\n","        super().__init__()\n","        assert isinstance(net, ReformerLM), 'generative trainer wrapper can only accept ReformerLM class'\n","        self.pad_value = pad_value\n","        self.ignore_index = ignore_index\n","\n","        self.net = Autopadder(net)\n","        self.max_seq_len = net.max_seq_len\n","\n","    @torch.no_grad()\n","    def generate(self, start_tokens, seq_len, eos_token = None, temperature = 1., filter_logits_fn = top_k, filter_thres = 0.9, **kwargs):\n","        was_training = self.net.training\n","        num_dims = len(start_tokens.shape)\n","\n","        if num_dims == 1:\n","            start_tokens = start_tokens[None, :]\n","\n","        b, t = start_tokens.shape\n","\n","        self.net.eval()\n","        out = start_tokens\n","        input_mask = kwargs.pop('input_mask', None)\n","\n","        if input_mask is None:\n","            input_mask = torch.full_like(out, True, dtype=torch.bool, device=out.device)\n","\n","        for _ in range(seq_len):\n","            x = out[:, -self.max_seq_len:]\n","            input_mask = input_mask[:, -self.max_seq_len:]\n","\n","            logits = self.net(x, input_mask=input_mask, **kwargs)[:, -1, :]\n","            filtered_logits = filter_logits_fn(logits, thres = filter_thres)\n","            probs = F.softmax(filtered_logits / temperature, dim=-1)\n","            sample = torch.multinomial(probs, 1)\n","\n","            out = torch.cat((out, sample), dim=-1)\n","            input_mask = F.pad(input_mask, (0, 1), value=True)\n","\n","            if eos_token is not None and (sample == eos_token).all():\n","                break\n","\n","        out = out[:, t:]\n","\n","        if num_dims == 1:\n","            out = out.squeeze(0)\n","\n","        self.net.train(was_training)\n","        return out\n","\n","    def forward(self, x, return_loss = False, **kwargs):\n","        pad = partial(pad_sequence, batch_first = True, padding_value = self.pad_value)\n","\n","        if not return_loss:\n","            if not isinstance(x, torch.Tensor):\n","                x = pad(x)\n","            return self.net(x, **kwargs)\n","\n","        if isinstance(x, torch.Tensor):\n","            xi = x[:, :-1]\n","            xo = x[:, 1:]\n","        else:\n","            xi = pad(list(map(lambda t: t[:-1], x)))\n","            xo = pad(list(map(lambda t: t[1:], x)))\n","\n","        out = self.net(xi, **kwargs)\n","\n","        loss = F.cross_entropy(out.transpose(1, 2), xo, ignore_index = self.ignore_index)\n","        return loss\n"],"metadata":{"id":"GEZJdMgkDrKE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","\n","ENC_PREFIX = 'enc_'\n","DEC_PREFIX = 'dec_'\n","\n","def group_dict_by_key(cond, d):\n","    return_val = [dict(),dict()]\n","    for key in d.keys():\n","        match = bool(cond(key))\n","        ind = int(not match)\n","        return_val[ind][key] = d[key]\n","    return (*return_val,)\n","\n","def string_begins_with(prefix, str):\n","    return bool(re.match(f'^{prefix}', str))\n","\n","def group_by_key_prefix(prefix, d):\n","    return group_dict_by_key(lambda x: string_begins_with(prefix, x), d)\n","\n","def group_by_key_prefix_and_remove_prefix(prefix, d):\n","    kwargs_with_prefix, kwargs = group_dict_by_key(lambda x: string_begins_with(prefix, x), d)\n","    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n","    return kwargs_without_prefix, kwargs\n","\n","def extract_enc_dec_kwargs(kwargs):\n","    enc_kwargs, kwargs = group_by_key_prefix_and_remove_prefix(ENC_PREFIX, kwargs)\n","    dec_kwargs, kwargs = group_by_key_prefix_and_remove_prefix(DEC_PREFIX, kwargs)\n","    return enc_kwargs, dec_kwargs, kwargs\n","\n","def extract_and_set_enc_dec_kwargs(kwargs):\n","    enc_kwargs, dec_kwargs, kwargs = extract_enc_dec_kwargs(kwargs)\n","    if 'input_mask' in enc_kwargs:\n","        dec_kwargs.setdefault('context_mask', enc_kwargs['input_mask'])\n","    return enc_kwargs, dec_kwargs, kwargs\n","\n","class ReformerEncDec(nn.Module):\n","    def __init__(self, dim, ignore_index = 0, pad_value = 0, **kwargs):\n","        super().__init__()\n","        enc_kwargs, dec_kwargs, _ = extract_enc_dec_kwargs(kwargs)\n","        \n","        assert 'return_embedding' not in enc_kwargs, 'you cannot manually set the return embeddings flag for the encoder'\n","        assert 'dim' not in dec_kwargs and 'dim' not in enc_kwargs, 'you must set the dim for both encoder and decoder'\n","\n","        enc_kwargs['dim'] = dec_kwargs['dim'] = dim\n","        enc_kwargs['return_embeddings'] = True\n","        dec_kwargs['causal'] = True\n","\n","        enc_kwargs.setdefault('bucket_size', 64)\n","        dec_kwargs.setdefault('bucket_size', enc_kwargs['bucket_size'] * 2)\n","\n","        enc = ReformerLM(**enc_kwargs)\n","        dec = ReformerLM(**dec_kwargs)\n","\n","        self.enc = TrainingWrapper(enc, ignore_index = ignore_index, pad_value = pad_value)\n","        self.dec = TrainingWrapper(dec, ignore_index = ignore_index, pad_value = pad_value)\n","\n","    def generate(self, seq_in, seq_out_start, seq_len, **kwargs):\n","        enc_kwargs, dec_kwargs, kwargs = extract_and_set_enc_dec_kwargs(kwargs)\n","        enc_keys = self.enc(seq_in, **enc_kwargs)\n","        return self.dec.generate(seq_out_start, seq_len, keys = enc_keys, **{**dec_kwargs, **kwargs})\n","\n","    def forward(self, seq_in, seq_out, return_loss = False, **kwargs):\n","        enc_kwargs, dec_kwargs, kwargs = extract_and_set_enc_dec_kwargs(kwargs)\n","        enc_keys = self.enc(seq_in, **enc_kwargs)\n","        return self.dec(seq_out, return_loss = return_loss, keys = enc_keys, **dec_kwargs)\n"],"metadata":{"id":"2Tcl7rwaDzDa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from local_attention import LocalAttention\n","from torch.autograd.function import Function\n","from torch.utils.checkpoint import get_device_states, set_device_states\n","\n","# following example for saving and setting rng here https://pytorch.org/docs/stable/_modules/torch/utils/checkpoint.html\n","class Deterministic(nn.Module):\n","    def __init__(self, net):\n","        super().__init__()\n","        self.net = net\n","        self.cpu_state = None\n","        self.cuda_in_fwd = None\n","        self.gpu_devices = None\n","        self.gpu_states = None\n","\n","    def record_rng(self, *args):\n","        self.cpu_state = torch.get_rng_state()\n","        if torch.cuda._initialized:\n","            self.cuda_in_fwd = True\n","            self.gpu_devices, self.gpu_states = get_device_states(*args)\n","\n","    def forward(self, *args, record_rng = False, set_rng = False, **kwargs):\n","        if record_rng:\n","            self.record_rng(*args)\n","\n","        if not set_rng:\n","            return self.net(*args, **kwargs)\n","\n","        rng_devices = []\n","        if self.cuda_in_fwd:\n","            rng_devices = self.gpu_devices\n","\n","        with torch.random.fork_rng(devices=rng_devices, enabled=True):\n","            torch.set_rng_state(self.cpu_state)\n","            if self.cuda_in_fwd:\n","                set_device_states(self.gpu_devices, self.gpu_states)\n","            return self.net(*args, **kwargs)\n","\n","# heavily inspired by https://github.com/RobinBruegger/RevTorch/blob/master/revtorch/revtorch.py\n","# once multi-GPU is confirmed working, refactor and send PR back to source\n","class ReversibleBlock(nn.Module):\n","    def __init__(self, f, g, depth=None, send_signal = False):\n","        super().__init__()\n","        self.f = Deterministic(f)\n","        self.g = Deterministic(g)\n","\n","        self.depth = depth\n","        self.send_signal = send_signal\n","\n","    def forward(self, x, f_args = {}, g_args = {}):\n","        x1, x2 = torch.chunk(x, 2, dim=2)\n","        y1, y2 = None, None\n","\n","        if self.send_signal:\n","            f_args['_reverse'] = g_args['_reverse'] = False\n","            f_args['_depth'] = g_args['_depth'] = self.depth\n","\n","        with torch.no_grad():\n","            y1 = x1 + self.f(x2, record_rng=self.training, **f_args)\n","            y2 = x2 + self.g(y1, record_rng=self.training, **g_args)\n","\n","        return torch.cat([y1, y2], dim=2)\n","\n","    def backward_pass(self, y, dy, f_args = {}, g_args = {}):\n","        y1, y2 = torch.chunk(y, 2, dim=2)\n","        del y\n","\n","        dy1, dy2 = torch.chunk(dy, 2, dim=2)\n","        del dy\n","\n","        if self.send_signal:\n","            f_args['_reverse'] = g_args['_reverse'] = True\n","            f_args['_depth'] = g_args['_depth'] = self.depth\n","\n","        with torch.enable_grad():\n","            y1.requires_grad = True\n","            gy1 = self.g(y1, set_rng=True, **g_args)\n","            torch.autograd.backward(gy1, dy2)\n","\n","        with torch.no_grad():\n","            x2 = y2 - gy1\n","            del y2, gy1\n","\n","            dx1 = dy1 + y1.grad\n","            del dy1\n","            y1.grad = None\n","\n","        with torch.enable_grad():\n","            x2.requires_grad = True\n","            fx2 = self.f(x2, set_rng=True, **f_args)\n","            torch.autograd.backward(fx2, dx1, retain_graph=True)\n","\n","        with torch.no_grad():\n","            x1 = y1 - fx2\n","            del y1, fx2\n","\n","            dx2 = dy2 + x2.grad\n","            del dy2\n","            x2.grad = None\n","\n","            x = torch.cat([x1, x2.detach()], dim=2)\n","            dx = torch.cat([dx1, dx2], dim=2)\n","\n","        return x, dx\n","\n","class IrreversibleBlock(nn.Module):\n","    def __init__(self, f, g):\n","        super().__init__()\n","        self.f = f\n","        self.g = g\n","\n","    def forward(self, x, f_args, g_args):\n","        x1, x2 = torch.chunk(x, 2, dim=2)\n","        y1 = x1 + self.f(x2, **f_args)\n","        y2 = x2 + self.g(y1, **g_args)\n","        return torch.cat([y1, y2], dim=2)\n","\n","class _ReversibleFunction(Function):\n","    @staticmethod\n","    def forward(ctx, x, blocks, kwargs):\n","        ctx.kwargs = kwargs\n","        for block in blocks:\n","            x = block(x, **kwargs)\n","        ctx.y = x.detach()\n","        ctx.blocks = blocks\n","        return x\n","\n","    @staticmethod\n","    def backward(ctx, dy):\n","        y = ctx.y\n","        kwargs = ctx.kwargs\n","        for block in ctx.blocks[::-1]:\n","            y, dy = block.backward_pass(y, dy, **kwargs)\n","        return dy, None, None\n","\n","class ReversibleSequence(nn.Module):\n","    def __init__(self, blocks, layer_dropout = 0., reverse_thres = 0, send_signal = False):\n","        super().__init__()\n","        self.layer_dropout = layer_dropout\n","        self.reverse_thres = reverse_thres\n","\n","        self.blocks = nn.ModuleList([ReversibleBlock(f, g, depth, send_signal) for depth, (f, g) in enumerate(blocks)])\n","        self.irrev_blocks = nn.ModuleList([IrreversibleBlock(f=f, g=g) for f, g in blocks])\n","\n","    def forward(self, x, arg_route = (True, False), **kwargs):\n","        reverse = x.shape[1] > self.reverse_thres\n","        blocks = self.blocks if reverse else self.irrev_blocks\n","\n","        if self.training and self.layer_dropout > 0:\n","            to_drop = torch.empty(len(self.blocks)).uniform_(0, 1) < self.layer_dropout\n","            blocks = [block for block, drop in zip(self.blocks, to_drop) if not drop]\n","            blocks = self.blocks[:1] if len(blocks) == 0 else blocks\n","\n","        f_args, g_args = map(lambda route: kwargs if route else {}, arg_route)\n","        block_kwargs = {'f_args': f_args, 'g_args': g_args}\n","\n","        if not reverse:\n","            for block in blocks:\n","                x = block(x, **block_kwargs)\n","            return x\n","\n","        return _ReversibleFunction.apply(x, blocks, block_kwargs)\n"],"metadata":{"id":"DJzVEBKWEKv6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","from torch.autograd import Function\n","from functools import partial, reduce, wraps\n","from itertools import chain\n","from operator import mul\n","from product_key_memory import PKM\n","from local_attention import LocalAttention\n","#from einops import rearrange, repeat\n","from einops import rearrange, repeat\n","#constants\n","\n","TOKEN_SELF_ATTN_VALUE = -5e4 # carefully set for half precision to work\n","\n","# helper fns\n","\n","def exists(val):\n","    return val is not None\n","\n","def sort_key_val(t1, t2, dim=-1):\n","    values, indices = t1.sort(dim=dim)\n","    t2 = t2.expand_as(t1)\n","    return values, t2.gather(dim, indices)\n","\n","def batched_index_select(values, indices):\n","    last_dim = values.shape[-1]\n","    return values.gather(1, indices[:, :, None].expand(-1, -1, last_dim))\n","\n","def process_inputs_chunk(fn, chunks=1, dim=0):\n","    def inner_fn(*args, **kwargs):\n","        keys, values, len_args = kwargs.keys(), kwargs.values(), len(args)\n","        chunked_args = list(zip(*map(lambda x: x.chunk(chunks, dim=dim), list(args) + list(values))))\n","        all_args = map(lambda x: (x[:len_args], dict(zip(keys, x[len_args:]))), chunked_args)\n","        outputs = [fn(*c_args, **c_kwargs) for c_args, c_kwargs in all_args]\n","        return tuple(map(lambda x: torch.cat(x, dim=dim), zip(*outputs)))\n","    return inner_fn\n","\n","def chunked_sum(tensor, chunks=1):\n","    *orig_size, last_dim = tensor.shape\n","    tensor = tensor.reshape(-1, last_dim)\n","    summed_tensors = [c.sum(dim=-1) for c in tensor.chunk(chunks, dim=0)]\n","    return torch.cat(summed_tensors, dim=0).reshape(orig_size)\n","\n","def default(val, default_val):\n","    return default_val if val is None else val\n","\n","def cast_tuple(x):\n","    return x if isinstance(x, tuple) else (x,)\n","\n","def max_neg_value(tensor):\n","    return -torch.finfo(tensor.dtype).max\n","\n","def cache_fn(f):\n","    cache = None\n","    @wraps(f)\n","    def cached_fn(*args, **kwargs):\n","        nonlocal cache\n","        if cache is not None:\n","            return cache\n","        cache = f(*args, **kwargs)\n","        return cache\n","    return cached_fn\n","\n","def cache_method_decorator(cache_attr, cache_namespace, reexecute = False):\n","    def inner_fn(fn):\n","        @wraps(fn)\n","        def wrapper(self, *args, key_namespace=None, fetch=False, set_cache=True, **kwargs):\n","            namespace_str = str(default(key_namespace, ''))\n","            _cache = getattr(self, cache_attr)\n","            _keyname = f'{cache_namespace}:{namespace_str}'\n","\n","            if fetch:\n","                val = _cache[_keyname]\n","                if reexecute:\n","                    fn(self, *args, **kwargs)\n","            else:\n","                val = fn(self, *args, **kwargs)\n","                if set_cache:\n","                    setattr(self, cache_attr, {**_cache, **{_keyname: val}})\n","            return val\n","        return wrapper\n","    return inner_fn\n","\n","def expand_dim(dim, k, t):\n","    t = t.unsqueeze(dim)\n","    expand_shape = [-1] * len(t.shape)\n","    expand_shape[dim] = k\n","    return t.expand(*expand_shape)\n","\n","def merge_dims(ind_from, ind_to, tensor):\n","    shape = list(tensor.shape)\n","    arr_slice = slice(ind_from, ind_to + 1)\n","    shape[arr_slice] = [reduce(mul, shape[arr_slice])]\n","    return tensor.reshape(*shape)\n","\n","def split_at_index(dim, index, t):\n","    pre_slices = (slice(None),) * dim\n","    l = (*pre_slices, slice(None, index))\n","    r = (*pre_slices, slice(index, None))\n","    return t[l], t[r]\n","\n","# helper classes\n","\n","class Always(nn.Module):\n","    def __init__(self, val):\n","        super().__init__()\n","        self.val = val\n","\n","    def forward(self, *args, **kwargs):\n","        return self.val\n","\n","class MatrixMultiply(nn.Module):\n","    def __init__(self, tensor, transpose = False, normalize = False):\n","        super().__init__()\n","        self.tensor = tensor\n","        self.transpose = transpose\n","        self.normalize = normalize\n","\n","    def forward(self, x):\n","        tensor = self.tensor\n","        if self.normalize:\n","            tensor = F.normalize(tensor, dim=-1)\n","        if self.transpose:\n","            tensor = tensor.t()\n","        return x @ tensor\n","\n","class ReZero(nn.Module):\n","    def __init__(self, fn):\n","        super().__init__()\n","        self.g = nn.Parameter(torch.zeros(1))\n","        self.fn = fn\n","\n","    def forward(self, x, **kwargs):\n","        return self.fn(x, **kwargs) * self.g\n","\n","class ScaleNorm(nn.Module):\n","    def __init__(self, dim, eps=1e-5):\n","        super().__init__()\n","        self.g = nn.Parameter(torch.ones(1))\n","        self.eps = eps\n","\n","    def forward(self, x):\n","        n = torch.norm(x, dim=-1, keepdim=True).clamp(min=self.eps)\n","        return x / n * self.g\n","\n","class PreNorm(nn.Module):\n","    def __init__(self, norm_class, dim, fn):\n","        super().__init__()\n","        self.norm = norm_class(dim)\n","        self.fn = fn\n","    def forward(self, x, **kwargs):\n","        x = self.norm(x)\n","        return self.fn(x, **kwargs)\n","\n","class Chunk(nn.Module):\n","    def __init__(self, chunks, fn, along_dim = -1):\n","        super().__init__()\n","        self.dim = along_dim\n","        self.chunks = chunks\n","        self.fn = fn\n","\n","    def forward(self, x, **kwargs):\n","        if self.chunks == 1:\n","            return self.fn(x, **kwargs)\n","        chunks = x.chunk(self.chunks, dim = self.dim)\n","        return torch.cat([self.fn(c, **kwargs) for c in chunks], dim = self.dim)\n","\n","# LSH attention as described in https://openreview.net/pdf?id=rkgNKkHtvB\n","# adapted from trax, stripped to what paper said needed to work\n","# namely that buckets need to be at least 64 with 8 rounds of hashing\n","# https://github.com/google/trax/blob/master/trax/layers/research/efficient_attention.py#L442\n","\n","class LSHAttention(nn.Module):\n","    def __init__( self,\n","                  dropout = 0.,\n","                  bucket_size = 64,\n","                  n_hashes = 8,\n","                  causal = False,\n","                  allow_duplicate_attention = True,\n","                  attend_across_buckets = True,\n","                  rehash_each_round = True,\n","                  drop_for_hash_rate = 0.0,\n","                  random_rotations_per_head = False,\n","                  return_attn = False):\n","        super().__init__()\n","        if dropout >= 1.0:\n","            raise ValueError('Dropout rates must be lower than 1.')\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.dropout_for_hash = nn.Dropout(drop_for_hash_rate)\n","\n","        assert rehash_each_round or allow_duplicate_attention, (\n","            'The setting {allow_duplicate_attention=False, rehash_each_round=False}'\n","            ' is not implemented.')\n","\n","        self.causal = causal\n","        self.bucket_size = bucket_size\n","\n","        self.n_hashes = n_hashes\n","\n","        self._allow_duplicate_attention = allow_duplicate_attention\n","        self._attend_across_buckets = attend_across_buckets\n","        self._rehash_each_round = rehash_each_round\n","        self._random_rotations_per_head = random_rotations_per_head\n","\n","        # will expend extra computation to return attention matrix\n","        self._return_attn = return_attn\n","\n","        # cache buckets for reversible network, reported by authors to make Reformer work at depth\n","        self._cache = {}\n","\n","    @cache_method_decorator('_cache', 'buckets', reexecute=True)\n","    def hash_vectors(self, n_buckets, vecs):\n","        batch_size = vecs.shape[0]\n","        device = vecs.device\n","\n","        # See https://arxiv.org/pdf/1509.02897.pdf\n","        # We sample a different random rotation for each round of hashing to\n","        # decrease the probability of hash misses.\n","        assert n_buckets % 2 == 0\n","\n","        rot_size = n_buckets\n","\n","        rotations_shape = (\n","            batch_size if self._random_rotations_per_head else 1,\n","            vecs.shape[-1],\n","            self.n_hashes if self._rehash_each_round else 1,\n","            rot_size // 2)\n","\n","        random_rotations = torch.randn(rotations_shape, dtype=vecs.dtype, device=device).expand(batch_size, -1, -1, -1)\n","\n","        dropped_vecs = self.dropout_for_hash(vecs)\n","        rotated_vecs = torch.einsum('btf,bfhi->bhti', dropped_vecs, random_rotations)\n","\n","        if self._rehash_each_round:\n","            # rotated_vectors size [batch,n_hash,seq_len,buckets]\n","            rotated_vecs = torch.cat([rotated_vecs, -rotated_vecs], dim=-1)\n","            buckets = torch.argmax(rotated_vecs, dim=-1)\n","        else:\n","            rotated_vecs = torch.cat([rotated_vecs, -rotated_vecs], dim=-1)\n","            # In this configuration, we map each item to the top self.n_hashes buckets\n","            rotated_vecs = torch.squeeze(rotated_vecs, 1)\n","            bucket_range = torch.arange(rotated_vecs.shape[-1], device=device)\n","            bucket_range = torch.reshape(bucket_range, (1, -1))\n","            bucket_range = bucket_range.expand_as(rotated_vecs)\n","\n","            _, buckets = sort_key_val(rotated_vecs, bucket_range, dim=-1)\n","            # buckets size [batch size, seq_len, buckets]\n","            buckets = buckets[... , -self.n_hashes:].transpose(1, 2)\n","\n","        # buckets is now (self.n_hashes, seq_len). Next we add offsets so that\n","        # bucket numbers from different hashing rounds don't overlap.\n","        offsets = torch.arange(self.n_hashes, device=device)\n","        offsets = torch.reshape(offsets * n_buckets, (1, -1, 1))\n","        buckets = torch.reshape(buckets + offsets, (batch_size, -1,))\n","        return buckets\n","\n","    def forward(self, qk, v, query_len = None, input_mask = None, input_attn_mask = None, pos_emb = None, **kwargs):\n","        batch_size, seqlen, dim, device = *qk.shape, qk.device\n","\n","        query_len = default(query_len, seqlen)\n","        is_reverse = kwargs.pop('_reverse', False)\n","        depth = kwargs.pop('_depth', None)\n","\n","        assert seqlen % (self.bucket_size * 2) == 0, f'Sequence length ({seqlen}) needs to be divisible by target bucket size  x 2 - {self.bucket_size * 2}'\n","\n","        n_buckets = seqlen // self.bucket_size\n","        buckets = self.hash_vectors(n_buckets, qk, key_namespace=depth, fetch=is_reverse, set_cache=self.training)\n","\n","        # We use the same vector as both a query and a key.\n","        assert int(buckets.shape[1]) == self.n_hashes * seqlen\n","\n","        total_hashes = self.n_hashes\n","\n","        ticker = torch.arange(total_hashes * seqlen, device=device).unsqueeze(0).expand_as(buckets)\n","        buckets_and_t = seqlen * buckets + (ticker % seqlen)\n","        buckets_and_t = buckets_and_t.detach()\n","\n","        # Hash-based sort (\"s\" at the start of variable names means \"sorted\")\n","        sbuckets_and_t, sticker = sort_key_val(buckets_and_t, ticker, dim=-1)\n","        _, undo_sort = sticker.sort(dim=-1)\n","        del ticker\n","\n","        sbuckets_and_t = sbuckets_and_t.detach()\n","        sticker = sticker.detach()\n","        undo_sort = undo_sort.detach()\n","\n","        if exists(pos_emb):\n","            qk = apply_rotary_pos_emb(qk, pos_emb)\n","\n","        st = (sticker % seqlen)\n","        sqk = batched_index_select(qk, st)\n","        sv = batched_index_select(v, st)\n","\n","        # Split off a \"bin\" axis so that attention only occurs within chunks.\n","        chunk_size = total_hashes * n_buckets\n","        bq_t = bkv_t = torch.reshape(st, (batch_size, chunk_size, -1))\n","        bqk = torch.reshape(sqk, (batch_size, chunk_size, -1, dim))\n","        bv = torch.reshape(sv, (batch_size, chunk_size, -1, dim))\n","\n","        # Hashing operates on unit-length vectors. Unnormalized query vectors are\n","        # fine because they effectively provide a learnable temperature for the\n","        # attention softmax, but normalizing keys is needed so that similarity for\n","        # the purposes of attention correctly corresponds to hash locality.\n","        bq = bqk\n","        bk = F.normalize(bqk, p=2, dim=-1).type_as(bq)\n","\n","        # Allow each chunk to attend within itself, and also one chunk back. Chunk\n","        # boundaries might occur in the middle of a sequence of items from the\n","        # same bucket, so this increases the chances of attending to relevant items.\n","        def look_one_back(x):\n","            x_extra = torch.cat([x[:, -1:, ...], x[:, :-1, ...]], dim=1)\n","            return torch.cat([x, x_extra], dim=2)\n","\n","        bk = look_one_back(bk)\n","        bv = look_one_back(bv)\n","        bkv_t = look_one_back(bkv_t)\n","\n","        # Dot-product attention.\n","        dots = torch.einsum('bhie,bhje->bhij', bq, bk) * (dim ** -0.5)\n","        masked_value = max_neg_value(dots)\n","\n","        # Mask for post qk attention logits of the input sequence\n","        if input_attn_mask is not None:\n","            input_attn_mask = F.pad(input_attn_mask, (0, seqlen - input_attn_mask.shape[-1], 0, seqlen - input_attn_mask.shape[-2]), value=True)\n","            dot_attn_indices = ((bq_t * seqlen)[:, :, :, None] + bkv_t[:, :, None, :])\n","            input_attn_mask = input_attn_mask.reshape(batch_size, -1)\n","            dot_attn_indices = dot_attn_indices.reshape(batch_size, -1)\n","            mask = input_attn_mask.gather(1, dot_attn_indices).reshape_as(dots)\n","            dots.masked_fill_(~mask, masked_value)\n","            del mask\n","\n","        # Input mask for padding in variable lengthed sequences\n","        if input_mask is not None:\n","            input_mask = F.pad(input_mask, (0, seqlen - input_mask.shape[1]), value=True)\n","            mq = input_mask.gather(1, st).reshape((batch_size, chunk_size, -1))\n","            mkv = look_one_back(mq)\n","            mask = mq[:, :, :, None] * mkv[:, :, None, :]\n","            dots.masked_fill_(~mask, masked_value)\n","            del mask\n","\n","        # Causal masking\n","        if self.causal:\n","            mask = bq_t[:, :, :, None] < bkv_t[:, :, None, :]\n","            if seqlen > query_len:\n","                mask = mask & (bkv_t[:, :, None, :] < query_len)\n","            dots.masked_fill_(mask, masked_value)\n","            del mask\n","\n","        # Mask out attention to self except when no other targets are available.\n","        self_mask = bq_t[:, :, :, None] == bkv_t[:, :, None, :]\n","        dots.masked_fill_(self_mask, TOKEN_SELF_ATTN_VALUE)\n","        del self_mask\n","\n","        # Mask out attention to other hash buckets.\n","        if not self._attend_across_buckets:\n","            bq_buckets = bkv_buckets = torch.reshape(sbuckets_and_t // seqlen, (batch_size, chunk_size, -1))\n","            bkv_buckets = look_one_back(bkv_buckets)\n","            bucket_mask = bq_buckets[:, :, :, None] != bkv_buckets[:, :, None, :]\n","            dots.masked_fill_(bucket_mask, masked_value)\n","            del bucket_mask\n","\n","        # Don't double-count query-key pairs across multiple rounds of hashing.\n","        # There are two possible strategies here. (1) The default is to count how\n","        # many times a query-key pair is repeated, and to lower its log-prob\n","        # correspondingly at each repetition. (2) When hard_k is set, the code\n","        # instead masks all but the first occurence of each query-key pair.\n","        if not self._allow_duplicate_attention:\n","            locs1 = undo_sort // bq_t.shape[-1]\n","            locs2 = (locs1 + 1) % chunk_size\n","            if not self._attend_across_buckets:\n","                locs1 = buckets * chunk_size + locs1\n","                locs2 = buckets * chunk_size + locs2\n","            locs = torch.cat([\n","                torch.reshape(locs1, (batch_size, total_hashes, seqlen)),\n","                torch.reshape(locs2, (batch_size, total_hashes, seqlen)),\n","            ], 1).permute((0, 2, 1))\n","\n","            slocs = batched_index_select(locs, st)\n","            b_locs = torch.reshape(slocs, (batch_size, chunk_size, -1, 2 * total_hashes))\n","\n","            b_locs1 = b_locs[:, :, :, None, :total_hashes]\n","\n","            bq_locs = b_locs1.expand(b_locs.shape[:3] + (2, total_hashes))\n","            bq_locs = torch.reshape(bq_locs, b_locs.shape)\n","            bkv_locs = look_one_back(b_locs)\n","\n","            dup_counts = (bq_locs[:, :, :, None, :] == bkv_locs[:, :, None, :, :])\n","            # for memory considerations, chunk summation of last dimension for counting duplicates\n","            dup_counts = chunked_sum(dup_counts, chunks=(total_hashes * batch_size))\n","            dup_counts = dup_counts.detach()\n","            assert dup_counts.shape == dots.shape\n","            dots = dots - torch.log(dup_counts + 1e-9)\n","            del dup_counts\n","\n","        # Softmax.\n","        dots_logsumexp = torch.logsumexp(dots, dim=-1, keepdim=True)\n","        dots = torch.exp(dots - dots_logsumexp).type_as(dots)\n","        dropped_dots = self.dropout(dots)\n","\n","        bo = torch.einsum('buij,buje->buie', dropped_dots, bv)\n","        so = torch.reshape(bo, (batch_size, -1, dim))\n","        slogits = torch.reshape(dots_logsumexp, (batch_size, -1,))\n","\n","        # unsort logits\n","        o = batched_index_select(so, undo_sort)\n","        logits = slogits.gather(1, undo_sort)\n","\n","        o = torch.reshape(o, (batch_size, total_hashes, seqlen, dim))\n","        logits = torch.reshape(logits, (batch_size, total_hashes, seqlen, 1))\n","\n","        if query_len != seqlen:\n","            query_slice = (slice(None), slice(None), slice(0, query_len))\n","            o, logits = o[query_slice], logits[query_slice]\n","\n","        probs = torch.exp(logits - torch.logsumexp(logits, dim=1, keepdim=True))\n","        out = torch.sum(o * probs, dim=1)\n","\n","        attn = torch.empty(0, device=device)\n","\n","        # return unsorted attention weights\n","        if self._return_attn:\n","            attn_unsort = ((bq_t * seqlen)[:, :, :, None] + bkv_t[:, :, None, :])\n","            attn_unsort = attn_unsort.view(batch_size * total_hashes, -1).long()\n","            unsorted_dots = torch.zeros(batch_size * total_hashes, seqlen * seqlen, device=device)\n","            unsorted_dots.scatter_add_(1, attn_unsort, dots.view_as(attn_unsort))\n","            del attn_unsort\n","            unsorted_dots = unsorted_dots.reshape(batch_size, total_hashes, seqlen, seqlen)\n","            attn = torch.sum(unsorted_dots[:, :, 0:query_len, :] * probs, dim=1)\n","\n","        # return output, attention matrix, and bucket distribution\n","        return out, attn, buckets\n","\n","# simple full attention\n","\n","class FullQKAttention(nn.Module):\n","    def __init__(self, causal = False, dropout = 0.):\n","        super().__init__()\n","        self.causal = causal\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, qk, v, query_len = None, input_mask = None, input_attn_mask = None, **kwargs):\n","        b, seq_len, dim = qk.shape\n","        query_len = default(query_len, seq_len)\n","        t = query_len\n","\n","        q = qk[:, 0:query_len]\n","        qk = F.normalize(qk, 2, dim=-1).type_as(q)\n","\n","        dot = torch.einsum('bie,bje->bij', q, qk) * (dim ** -0.5)\n","\n","        # qk attention requires tokens not attend to self\n","        i = torch.arange(t)\n","        dot[:, i, i] = TOKEN_SELF_ATTN_VALUE\n","        masked_value = max_neg_value(dot)\n","\n","        # Input mask for padding in variable lengthed sequences\n","        if input_mask is not None:\n","            mask = input_mask[:, 0:query_len, None] * input_mask[:, None, :]\n","            mask = F.pad(mask, (0, seq_len - mask.shape[-1]), value=True)\n","            dot.masked_fill_(~mask, masked_value)\n","\n","        # Mask for post qk attention logits of the input sequence\n","        if input_attn_mask is not None:\n","            input_attn_mask = F.pad(input_attn_mask, (0, seq_len - input_attn_mask.shape[-1]), value=True)\n","            dot.masked_fill_(~input_attn_mask, masked_value)\n","\n","        if self.causal:\n","            i, j = torch.triu_indices(t, t, 1)\n","            dot[:, i, j] = masked_value\n","\n","        dot = dot.softmax(dim=-1)\n","        dot = self.dropout(dot)\n","\n","        out = torch.einsum('bij,bje->bie', dot, v)\n","\n","        return out, dot, torch.empty(0)\n","\n","# Shared qk attention, using either full or LSH attention\n","\n","class LSHSelfAttention(nn.Module):\n","    def __init__(self, dim, heads = 1, bucket_size = 4, n_hashes = 4, causal = False, dim_head = None, attn_chunks = 1, random_rotations_per_head = False, attend_across_buckets = True, allow_duplicate_attention = True, num_mem_kv = 0, one_value_head = False, use_full_attn = False, full_attn_thres = None, return_attn = False, post_attn_dropout = 0., dropout = 0., n_local_attn_heads = 0, **kwargs):\n","        super().__init__()\n","        assert dim_head or (dim % heads) == 0, 'dimensions must be divisible by number of heads'\n","        assert n_local_attn_heads < heads, 'local attention heads must be less than number of heads'\n","\n","        dim_head = default(dim_head, dim // heads)\n","        dim_heads = dim_head * heads\n","\n","        self.dim = dim\n","        self.heads = heads\n","        self.dim_head = dim_head\n","        self.attn_chunks = default(attn_chunks, 1)\n","\n","        self.v_head_repeats = (heads if one_value_head else 1)\n","        v_dim = dim_heads // self.v_head_repeats\n","\n","        self.toqk = nn.Linear(dim, dim_heads, bias = False)\n","        self.tov = nn.Linear(dim, v_dim, bias = False)\n","        self.to_out = nn.Linear(dim_heads, dim)\n","\n","        self.bucket_size = bucket_size\n","        self.lsh_attn = LSHAttention(bucket_size=bucket_size, n_hashes=n_hashes, causal=causal, random_rotations_per_head=random_rotations_per_head, attend_across_buckets = attend_across_buckets,  allow_duplicate_attention = allow_duplicate_attention, return_attn = return_attn, dropout = dropout, **kwargs)\n","        self.full_attn = FullQKAttention(causal=causal, dropout=dropout)\n","        self.post_attn_dropout = nn.Dropout(post_attn_dropout)\n","\n","        self.use_full_attn = use_full_attn\n","        self.full_attn_thres = default(full_attn_thres, bucket_size)\n","\n","        self.num_mem_kv = num_mem_kv\n","        self.mem_kv = nn.Parameter(torch.randn(1, num_mem_kv, dim, requires_grad=True)) if num_mem_kv > 0 else None\n","\n","        self.n_local_attn_heads = n_local_attn_heads\n","        self.local_attn = LocalAttention(window_size=bucket_size * 2, causal=causal, dropout=dropout, shared_qk=True, look_forward=(1 if not causal else 0))\n","\n","        self.callback = None\n","\n","    def forward(self, x, keys = None, input_mask = None, input_attn_mask = None, context_mask = None, pos_emb = None, **kwargs):\n","        device, dtype = x.device, x.dtype\n","        b, t, e, h, dh, m, l_h = *x.shape, self.heads, self.dim_head, self.num_mem_kv, self.n_local_attn_heads\n","\n","        mem_kv = default(self.mem_kv, torch.empty(b, 0, e, dtype=dtype, device=device))\n","        mem = mem_kv.expand(b, m, -1)\n","\n","        keys = default(keys, torch.empty(b, 0, e, dtype=dtype, device=device))\n","        c = keys.shape[1]\n","\n","        kv_len = t + m + c\n","        use_full_attn = self.use_full_attn or kv_len <= self.full_attn_thres\n","\n","        x = torch.cat((x, mem, keys), dim=1)\n","        qk = self.toqk(x)\n","        v = self.tov(x)\n","        v = v.repeat(1, 1, self.v_head_repeats)\n","\n","        def merge_heads(v):\n","            return v.view(b, kv_len, h, -1).transpose(1, 2)\n","\n","        def split_heads(v):\n","            return v.view(b, h, t, -1).transpose(1, 2).contiguous()\n","\n","        merge_batch_and_heads = partial(merge_dims, 0, 1)\n","\n","        qk, v = map(merge_heads, (qk, v))\n","\n","        has_local = l_h > 0\n","        lsh_h = h - l_h\n","\n","        split_index_fn = partial(split_at_index, 1, l_h)\n","        (lqk, qk), (lv, v) = map(split_index_fn, (qk, v))\n","        lqk, qk, lv, v = map(merge_batch_and_heads, (lqk, qk, lv, v))\n","\n","        masks = {}\n","        if input_mask is not None or context_mask is not None:\n","            default_mask = torch.tensor([True], device=device)\n","            i_mask = default(input_mask, default_mask.expand(b, t))\n","            m_mask = default_mask.expand(b, m)\n","            c_mask = default(context_mask, default_mask.expand(b, c))\n","            mask = torch.cat((i_mask, m_mask, c_mask), dim=1)\n","            mask = merge_batch_and_heads(expand_dim(1, lsh_h, mask))\n","            masks['input_mask'] = mask\n","\n","        if input_attn_mask is not None:\n","            input_attn_mask = merge_batch_and_heads(expand_dim(1, lsh_h, input_attn_mask))\n","            masks['input_attn_mask'] = input_attn_mask\n","\n","        attn_fn = self.lsh_attn if not use_full_attn else self.full_attn\n","        partial_attn_fn = partial(attn_fn, query_len = t, pos_emb = pos_emb, **kwargs)\n","        attn_fn_in_chunks = process_inputs_chunk(partial_attn_fn, chunks = self.attn_chunks)\n","\n","        out, attn, buckets = attn_fn_in_chunks(qk, v, **masks)\n","\n","        if self.callback is not None:\n","            self.callback(attn.reshape(b, lsh_h, t, -1), buckets.reshape(b, lsh_h, -1))\n","\n","        if has_local:\n","            lqk, lv = lqk[:, :t], lv[:, :t]\n","            local_out = self.local_attn(lqk, lqk, lv, input_mask=input_mask)\n","            local_out = local_out.reshape(b, l_h, t, -1)\n","            out = out.reshape(b, lsh_h, t, -1)\n","            out = torch.cat((local_out, out), dim=1)\n","\n","        out = split_heads(out).view(b, t, -1)\n","        out = self.to_out(out)\n","        return self.post_attn_dropout(out)\n","\n","# feed forward\n","\n","class GELU_(nn.Module):\n","    def forward(self, x):\n","        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n","\n","GELU = nn.GELU if hasattr(nn, 'GELU') else GELU_\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, dim, mult = 4, dropout = 0., activation = None, glu = False):\n","        super().__init__()\n","        activation = default(activation, GELU)\n","\n","        self.glu = glu\n","        self.w1 = nn.Linear(dim, dim * mult * (2 if glu else 1))\n","        self.act = activation()\n","        self.dropout = nn.Dropout(dropout)\n","        self.w2 = nn.Linear(dim * mult, dim)\n","\n","    def forward(self, x, **kwargs):\n","        if not self.glu:\n","            x = self.w1(x)\n","            x = self.act(x)\n","        else:\n","            x, v = self.w1(x).chunk(2, dim=-1)\n","            x = self.act(x) * v\n","\n","        x = self.dropout(x)\n","        x = self.w2(x)\n","        return x\n","\n","# reformer lm\n","\n","class Reformer(nn.Module):\n","    def __init__(self, dim, depth, heads = 8, dim_head = None, bucket_size = 32, n_hashes = 4, ff_chunks = 50, attn_chunks = None, causal = False, weight_tie = False, lsh_dropout = 0., ff_dropout = 0., ff_activation = None, ff_mult = 4, ff_glu = False, post_attn_dropout = 0., layer_dropout = 0., lsh_attend_across_buckets = True, lsh_allow_duplicate_attention = True, random_rotations_per_head = False, use_scale_norm = False, use_rezero = False, use_full_attn = False, full_attn_thres = 0, reverse_thres = 0, num_mem_kv = 0, one_value_head = False, n_local_attn_heads = 0, pkm_layers = tuple(), pkm_num_keys = 128):\n","        super().__init__()\n","        self.dim = dim\n","        self.depth = depth\n","\n","        self.bucket_size = bucket_size\n","        self.num_mem_kv = num_mem_kv\n","\n","        self.full_attn_thres = full_attn_thres\n","\n","        get_attn = lambda: LSHSelfAttention(dim, heads, bucket_size, n_hashes, causal = causal, dim_head = dim_head, dropout = lsh_dropout, post_attn_dropout = post_attn_dropout, attn_chunks = attn_chunks, allow_duplicate_attention = lsh_allow_duplicate_attention, attend_across_buckets = lsh_attend_across_buckets, random_rotations_per_head = random_rotations_per_head, num_mem_kv = num_mem_kv, use_full_attn = use_full_attn, full_attn_thres = full_attn_thres, one_value_head = one_value_head, n_local_attn_heads = n_local_attn_heads)\n","        get_ff = lambda: Chunk(ff_chunks, FeedForward(dim, dropout = ff_dropout, activation = ff_activation, mult = ff_mult, glu = ff_glu), along_dim = -2)\n","        get_pkm = lambda: PKM(dim, num_keys = pkm_num_keys)\n","\n","        if weight_tie:\n","            get_attn, get_ff, get_pkm = map(cache_fn, (get_attn, get_ff, get_pkm))\n","\n","        blocks = []\n","\n","        norm_type = ScaleNorm if use_scale_norm else nn.LayerNorm\n","\n","        residual_fn_wrapper = ReZero if use_rezero else partial(PreNorm, norm_type, dim)\n","\n","        for ind in range(depth):\n","            layer_num = ind + 1\n","            use_pkm = layer_num in cast_tuple(pkm_layers)\n","            parallel_net = None\n","\n","            attn = get_attn()\n","\n","            if use_pkm:\n","                parallel_net = get_pkm()\n","            else:\n","                parallel_net = get_ff()\n","\n","            f = residual_fn_wrapper(attn)\n","            g = residual_fn_wrapper(parallel_net)\n","\n","            blocks.append(nn.ModuleList([f, g]))\n","\n","        self.layers = ReversibleSequence(nn.ModuleList(blocks), layer_dropout = layer_dropout, reverse_thres = reverse_thres, send_signal = True)\n","\n","    def forward(self, x, **kwargs):\n","        x = torch.cat([x, x], dim = -1)\n","        x = self.layers(x, **kwargs)\n","        return torch.stack(x.chunk(2, dim=-1)).mean(dim=0)\n","\n"],"metadata":{"id":"kig0SKxXD1Td"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","class EncoderLayer(nn.Module):\n","    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation=\"relu\"):\n","        super(EncoderLayer, self).__init__()\n","        d_ff = d_ff or 4 * d_model\n","        self.attention = attention\n","        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n","        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","        self.activation = F.relu if activation == \"relu\" else F.gelu\n","\n","    def forward(self, x, attn_mask=None):\n","        new_x, attn = self.attention(\n","            x, x, x,\n","            attn_mask=attn_mask\n","        )\n","        x = x + self.dropout(new_x)\n","\n","        y = x = self.norm1(x)\n","        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n","        y = self.dropout(self.conv2(y).transpose(-1, 1))\n","\n","        return self.norm2(x + y), attn\n","\n","class DecoderLayer(nn.Module):\n","    def __init__(self, self_attention, cross_attention, d_model, d_ff=None,\n","                 dropout=0.1, activation=\"relu\"):\n","        super(DecoderLayer, self).__init__()\n","        d_ff = d_ff or 4 * d_model\n","        self.self_attention = self_attention\n","        self.cross_attention = cross_attention\n","        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n","        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.norm3 = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","        self.activation = F.relu if activation == \"relu\" else F.gelu\n","\n","    def forward(self, x, cross, x_mask=None, cross_mask=None):\n","        x = x + self.dropout(self.self_attention(\n","            x, x, x,\n","            attn_mask=x_mask\n","        )[0])\n","        x = self.norm1(x)\n","\n","        x = x + self.dropout(self.cross_attention(\n","            x, cross, cross,\n","            attn_mask=cross_mask\n","        )[0])\n","\n","        y = x = self.norm2(x)\n","        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n","        y = self.dropout(self.conv2(y).transpose(-1, 1))\n","\n","        return self.norm3(x + y)\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n","        super(Encoder, self).__init__()\n","        self.attn_layers = nn.ModuleList(attn_layers)\n","        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n","        self.norm = norm_layer\n","\n","    def forward(self, x, attn_mask=None):\n","        # x [B, L, D]\n","        attns = []\n","        if self.conv_layers is not None:\n","            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):\n","                x, attn = attn_layer(x, attn_mask=attn_mask)\n","                x = conv_layer(x)\n","                attns.append(attn)\n","            x, attn = self.attn_layers[-1](x)\n","            attns.append(attn)\n","        else:\n","            for attn_layer in self.attn_layers:\n","                x, attn = attn_layer(x, attn_mask=attn_mask)\n","                attns.append(attn)\n","\n","        if self.norm is not None:\n","            x = self.norm(x)\n","\n","        return x"],"metadata":{"id":"WpPqhx7I42K5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ReformerLayer(nn.Module):\n","    def __init__(self, attention, d_model, n_heads, d_keys=None,\n","                 d_values=None, causal=True, bucket_size=2, n_hashes=2):\n","        super().__init__()\n","        self.bucket_size = bucket_size\n","        self.attn = LSHSelfAttention(\n","            dim=d_model,\n","            heads=n_heads,\n","            bucket_size=bucket_size,\n","            n_hashes=n_hashes,\n","            causal=causal\n","        )\n","\n","    def fit_length(self, queries):\n","        # inside reformer: assert N % (bucket_size * 2) == 0\n","        B, N, C = queries.shape\n","        if N % (self.bucket_size * 2) == 0:\n","            return queries\n","        else:\n","            # fill the time series\n","            fill_len = (self.bucket_size * 2) - (N % (self.bucket_size * 2))\n","            return torch.cat([queries, torch.zeros([B, fill_len, C]).to(queries.device)], dim=1)\n","\n","    def forward(self, queries, keys, values, attn_mask):\n","        # in Reformer: defalut queries=keys\n","        B, N, C = queries.shape\n","        queries = self.attn(self.fit_length(queries))[:, :N, :]\n","        return queries, None\n","\n","\n","class DecisionReformer(nn.Module):\n","    def __init__(self, state_dim, act_dim, n_blocks, h_dim, context_len, \n","                 n_heads, drop_p, max_timestep=4096):\n","        super().__init__()\n","        \n","        self.state_dim = state_dim\n","        self.act_dim = act_dim\n","        self.h_dim = h_dim\n","        input_seq_len = 3 * context_len\n","        #self.projection = nn.Linear(configs.d_model, configs.c_out, bias=True)\n","\n","        self.transformer = Encoder(\n","            [\n","                EncoderLayer(\n","                    ReformerLayer(None, h_dim, n_heads, bucket_size=64,\n","                                  n_hashes=8),\n","                    h_dim,\n","                    4*h_dim,\n","                    dropout=drop_p,\n","                    activation=\"gelu\"\n","                ) for l in range(n_blocks)\n","            ],\n","            norm_layer=torch.nn.LayerNorm(h_dim)\n","        )\n","        #self.projection = nn.Linear(h_dim, configs.c_out, bias=True)\n","\n","\n","        ### projection heads (project to embedding)\n","        self.embed_ln = nn.LayerNorm(h_dim)\n","        self.embed_timestep = nn.Embedding(max_timestep, h_dim)\n","        self.embed_rtg = torch.nn.Linear(1, h_dim)\n","        self.embed_state = torch.nn.Linear(state_dim, h_dim)\n","        \n","        # # discrete actions\n","        # self.embed_action = torch.nn.Embedding(act_dim, h_dim)\n","        # use_action_tanh = False # False for discrete actions\n","\n","        # continuous actions\n","        self.embed_action = torch.nn.Linear(act_dim, h_dim)\n","        use_action_tanh = True # True for continuous actions\n","        \n","        ### prediction heads\n","        self.predict_rtg = torch.nn.Linear(h_dim, 1)\n","        self.predict_state = torch.nn.Linear(h_dim, state_dim)\n","        self.predict_action = nn.Sequential(\n","            *([nn.Linear(h_dim, act_dim)] + ([nn.Tanh()] if use_action_tanh else []))\n","        )\n","\n","\n","    def forward(self, timesteps, states, actions, returns_to_go):\n","\n","        B, T, _ = states.shape\n","\n","        time_embeddings = self.embed_timestep(timesteps)\n","\n","        # time embeddings are treated similar to positional embeddings\n","        state_embeddings = self.embed_state(states) + time_embeddings\n","        action_embeddings = self.embed_action(actions) + time_embeddings\n","        returns_embeddings = self.embed_rtg(returns_to_go) + time_embeddings\n","\n","        # stack rtg, states and actions and reshape sequence as\n","        # (r1, s1, a1, r2, s2, a2 ...)\n","        h = torch.stack(\n","            (returns_embeddings, state_embeddings, action_embeddings), dim=1\n","        ).permute(0, 2, 1, 3).reshape(B, 3 * T, self.h_dim)\n","\n","        h = self.embed_ln(h)\n","        \n","        # transformer and prediction\n","        h = self.transformer(h)\n","        \n","        # get h reshaped such that its size = (B x 3 x T x h_dim) and\n","        # h[:, 0, t] is conditioned on r_0, s_0, a_0 ... r_t\n","        # h[:, 1, t] is conditioned on r_0, s_0, a_0 ... r_t, s_t\n","        # h[:, 2, t] is conditioned on r_0, s_0, a_0 ... r_t, s_t, a_t\n","        h = h.reshape(B, T, 3, self.h_dim).permute(0, 2, 1, 3)\n","\n","        # get predictions\n","        return_preds = self.predict_rtg(h[:,2])     # predict next rtg given r, s, a\n","        state_preds = self.predict_state(h[:,2])    # predict next state given r, s, a\n","        action_preds = self.predict_action(h[:,1])  # predict action given r, s\n","    \n","        return state_preds, action_preds, return_preds"],"metadata":{"id":"exLAAbRZnMMp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MultiplyByScalarLayer(nn.Module):\n","    # A simple layer to multiply all entries by a constant scalar value. Needed since action inputs are not normalized in\n","    # many environments and tanh is then critical, unlike in D4RL where actions are in [-1, 1].\n","    # scalar value should be absolute max possible action value.\n","\n","    def __init__(self, scalar):\n","        super(MultiplyByScalarLayer, self).__init__()\n","        self.scalar = scalar\n","\n","    def forward(self, tensors):\n","        result = torch.clone(tensors)\n","        for ind in range(result.shape[0]):\n","            result[ind] = torch.mul(result[ind], self.scalar)\n","        return result\n","        \n","class TrajectoryModel(nn.Module):\n","\n","    def __init__(self, state_dim, act_dim, max_length=None):\n","        super().__init__()\n","\n","        self.state_dim = state_dim\n","        self.act_dim = act_dim\n","        self.max_length = max_length\n","\n","    def forward(self, states, actions, rewards, masks=None, attention_mask=None):\n","        # \"masked\" tokens or unspecified inputs can be passed in as None\n","        return None, None, None\n","\n","    def get_action(self, states, actions, rewards, **kwargs):\n","        # these will come as tensors on the correct device\n","        return torch.zeros_like(actions[-1])"],"metadata":{"id":"5H0bavgNByQi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.types import Device\n","\n","class DecisionLSTM(TrajectoryModel):\n","\n","    def __init__(\n","        self,\n","        state_dim,\n","        act_dim,\n","        hidden_size,\n","        max_length=None,\n","        max_ep_len=4096,\n","        action_tanh=True,\n","        scalar=1.,\n","        **kwargs    \n","    ):\n","        super().__init__(state_dim, act_dim, max_length=max_length) \n","       \n","        self.hidden_size = hidden_size\n","\n","        # LSTM\n","        self.lstm = nn.LSTM(\n","            input_size=hidden_size,\n","            hidden_size=hidden_size,\n","            **kwargs\n","        ).to(device)\n","\n","        self.embed_return = torch.nn.Linear(1, hidden_size)\n","        self.embed_state = torch.nn.Linear(state_dim, hidden_size)\n","        self.embed_action = torch.nn.Linear(act_dim, hidden_size)\n","\n","        self.embed_ln = nn.LayerNorm(hidden_size)\n","\n","        # note: we don't predict states or returns for the paper\n","        self.predict_state = nn.Linear(hidden_size, self.state_dim)\n","        self.predict_action = nn.Sequential(\n","            *([nn.Linear(hidden_size, self.act_dim)] + ([nn.Tanh()] if action_tanh else []) + ([MultiplyByScalarLayer(scalar=scalar)] if action_tanh else []))\n","        )\n","        self.predict_return = nn.Linear(hidden_size, 1)\n","\n","    def forward(self, states, actions,  returns_to_go, timesteps, rewards=None, attention_mask=None):\n","        batch_size, seq_length = states.shape[0], states.shape[1]\n","\n","        # embed each modality with a different head\n","        state_embeddings = self.embed_state(states.to(torch.float32))\n","        action_embeddings = self.embed_action(actions.to(torch.float32))\n","        returns_embeddings = self.embed_return(returns_to_go.to(torch.float32))  \n","\n","        # this makes the sequence look like (R_1, s_1, a_1, R_2, s_2, a_2, ...)\n","        # which works nice in an autoregressive sense since states predict actions\n","        stacked_inputs = torch.stack(\n","            (returns_embeddings, state_embeddings, action_embeddings), dim=1\n","        ).permute(0, 2, 1, 3).reshape(batch_size, 3*seq_length, int(self.hidden_size))\n","        stacked_inputs = self.embed_ln(stacked_inputs)     \n","\n","        h_0 = Variable(torch.zeros(3, 3*seq_length, self.hidden_size)).to(device)\n","        c_0 = Variable(torch.zeros(3, 3*seq_length, self.hidden_size)).to(device)\n","        \n","        self.lstm.cuda()\n","\n","        stacked_inputs = stacked_inputs.to(device)\n","        lstm_outputs, _ = self.lstm(\n","            stacked_inputs,\n","            (h_0, c_0)\n","        )\n","        lstm_outputs = lstm_outputs.to(device)\n","        x = lstm_outputs\n","\n","        # reshape x so that the second dimension corresponds to the original\n","        # returns (0), states (1), or actions (2); i.e. x[:,1,t] is the token for s_t\n","        x = x.reshape(batch_size, seq_length, 3, self.hidden_size).permute(0, 2, 1, 3)\n","\n","        # get predictions\n","        return_preds = self.predict_return(x[:,2])  # predict next return given state and action\n","        state_preds = self.predict_state(x[:,2])    # predict next state given state and action\n","        action_preds = self.predict_action(x[:,1])  # predict next action given state\n","\n","        return state_preds, action_preds, return_preds\n","\n","    def get_action(self, states, actions, rewards, returns_to_go, timesteps, **kwargs):\n","        # we don't care about the past rewards in this model\n","\n","        states = states.reshape(1, -1, self.state_dim)\n","        actions = actions.reshape(1, -1, self.act_dim)\n","        returns_to_go = returns_to_go.reshape(1, -1, 1)\n","        timesteps = timesteps.reshape(1, -1)\n","\n","        if self.max_length is not None:\n","            attention_mask = None\n","            states = states[:,-self.max_length:]\n","            actions = actions[:,-self.max_length:]\n","            returns_to_go = returns_to_go[:,-self.max_length:]\n","            timesteps = timesteps[:,-self.max_length:]\n","\n","            # pad all tokens to sequence length\n","            states = torch.cat(\n","                [torch.zeros((states.shape[0], self.max_length-states.shape[1], self.state_dim), device=states.device), states],\n","                dim=1).to(dtype=torch.float32)\n","            actions = torch.cat(\n","                [torch.zeros((actions.shape[0], self.max_length - actions.shape[1], self.act_dim),\n","                             device=actions.device), actions],\n","                dim=1).to(dtype=torch.float32)\n","            returns_to_go = torch.cat(\n","                [torch.zeros((returns_to_go.shape[0], self.max_length-returns_to_go.shape[1], 1), device=returns_to_go.device), returns_to_go],\n","                dim=1).to(dtype=torch.float32)\n","            timesteps = torch.cat(\n","                [torch.zeros((timesteps.shape[0], self.max_length-timesteps.shape[1]), device=timesteps.device), timesteps],\n","                dim=1).to(dtype=torch.long)\n","        else:\n","            attention_mask = None\n","\n","        _, action_preds, return_preds = self.forward(\n","            states, actions, None, returns_to_go, timesteps, attention_mask=attention_mask, **kwargs)\n","\n","        return action_preds[0,-1]"],"metadata":{"id":"oNi3W8gLBs5t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# DT"],"metadata":{"id":"N3mNpQT1li9C"}},{"cell_type":"code","source":["\n","\"\"\"\n","this extremely minimal GPT model is based on:\n","Misha Laskin's tweet: \n","https://twitter.com/MishaLaskin/status/1481767788775628801?cxt=HHwWgoCzmYD9pZApAAAA\n","\n","and its corresponding notebook:\n","https://colab.research.google.com/drive/1NUBqyboDcGte5qAJKOl8gaJC28V_73Iv?usp=sharing\n","\n","the above colab has a bug while applying masked_fill which is fixed in the\n","following code\n","\n","\"\"\"\n","\n","\"\"\"\n","NOT the same model as the official decision transformer\n","\"\"\"\n","\n","class MaskedCausalAttention(nn.Module):\n","    def __init__(self, h_dim, max_T, n_heads, drop_p):\n","        super().__init__()\n","\n","        self.n_heads = n_heads\n","        self.max_T = max_T\n","\n","        self.q_net = nn.Linear(h_dim, h_dim)\n","        self.k_net = nn.Linear(h_dim, h_dim)\n","        self.v_net = nn.Linear(h_dim, h_dim)\n","\n","        self.proj_net = nn.Linear(h_dim, h_dim)\n","\n","        self.att_drop = nn.Dropout(drop_p)\n","        self.proj_drop = nn.Dropout(drop_p)\n","\n","        ones = torch.ones((max_T, max_T))\n","        mask = torch.tril(ones).view(1, 1, max_T, max_T)\n","\n","        # register buffer makes sure mask does not get updated\n","        # during backpropagation\n","        self.register_buffer('mask',mask)\n","\n","    def forward(self, x):\n","        B, T, C = x.shape # batch size, seq length, h_dim * n_heads\n","\n","        N, D = self.n_heads, C // self.n_heads # N = num heads, D = attention dim\n","\n","        # rearrange q, k, v as (B, N, T, D)\n","        q = self.q_net(x).view(B, T, N, D).transpose(1,2)\n","        k = self.k_net(x).view(B, T, N, D).transpose(1,2)\n","        v = self.v_net(x).view(B, T, N, D).transpose(1,2)\n","\n","        # weights (B, N, T, T)\n","        weights = q @ k.transpose(2,3) / math.sqrt(D)\n","        # causal mask applied to weights\n","        weights = weights.masked_fill(self.mask[...,:T,:T] == 0, float('-inf'))\n","        # normalize weights, all -inf -> 0 after softmax\n","        normalized_weights = F.softmax(weights, dim=-1)\n","\n","        # attention (B, N, T, D)\n","        attention = self.att_drop(normalized_weights @ v)\n","\n","        # gather heads and project (B, N, T, D) -> (B, T, N*D)\n","        attention = attention.transpose(1, 2).contiguous().view(B,T,N*D)\n","\n","        out = self.proj_drop(self.proj_net(attention))\n","        return out\n","\n","\n","class Block(nn.Module):\n","    def __init__(self, h_dim, max_T, n_heads, drop_p):\n","        super().__init__()\n","        self.attention = MaskedCausalAttention(h_dim, max_T, n_heads, drop_p)\n","        self.mlp = nn.Sequential(\n","                nn.Linear(h_dim, 4*h_dim),\n","                nn.GELU(),\n","                nn.Linear(4*h_dim, h_dim),\n","                nn.Dropout(drop_p),\n","            )\n","        self.ln1 = nn.LayerNorm(h_dim)\n","        self.ln2 = nn.LayerNorm(h_dim)\n","\n","    def forward(self, x):\n","        # Attention -> LayerNorm -> MLP -> LayerNorm\n","        x = x + self.attention(x) # residual\n","        x = self.ln1(x)\n","        x = x + self.mlp(x) # residual\n","        x = self.ln2(x)\n","        return x\n","\n","\n","class DecisionTransformer(nn.Module):\n","    def __init__(self, state_dim, act_dim, n_blocks, h_dim, context_len, \n","                 n_heads, drop_p, max_timestep=4096):\n","        super().__init__()\n","\n","        self.state_dim = state_dim\n","        self.act_dim = act_dim\n","        self.h_dim = h_dim\n","\n","        ### transformer blocks\n","        input_seq_len = 3 * context_len\n","        blocks = [Block(h_dim, input_seq_len, n_heads, drop_p) for _ in range(n_blocks)]\n","        self.transformer = nn.Sequential(*blocks)\n","\n","        ### projection heads (project to embedding)\n","        self.embed_ln = nn.LayerNorm(h_dim)\n","        self.embed_timestep = nn.Embedding(max_timestep, h_dim)\n","        self.embed_rtg = torch.nn.Linear(1, h_dim)\n","        self.embed_state = torch.nn.Linear(state_dim, h_dim)\n","        \n","        # # discrete actions\n","        # self.embed_action = torch.nn.Embedding(act_dim, h_dim)\n","        # use_action_tanh = False # False for discrete actions\n","\n","        # continuous actions\n","        self.embed_action = torch.nn.Linear(act_dim, h_dim)\n","        use_action_tanh = True # True for continuous actions\n","        \n","        ### prediction heads\n","        self.predict_rtg = torch.nn.Linear(h_dim, 1)\n","        self.predict_state = torch.nn.Linear(h_dim, state_dim)\n","        self.predict_action = nn.Sequential(\n","            *([nn.Linear(h_dim, act_dim)] + ([nn.Tanh()] if use_action_tanh else []))\n","        )\n","\n","\n","    def forward(self, timesteps, states, actions, returns_to_go):\n","\n","        B, T, _ = states.shape\n","\n","        time_embeddings = self.embed_timestep(timesteps)\n","\n","        # time embeddings are treated similar to positional embeddings\n","        state_embeddings = self.embed_state(states) + time_embeddings\n","        action_embeddings = self.embed_action(actions) + time_embeddings\n","        returns_embeddings = self.embed_rtg(returns_to_go) + time_embeddings\n","\n","        # stack rtg, states and actions and reshape sequence as\n","        # (r1, s1, a1, r2, s2, a2 ...)\n","        h = torch.stack(\n","            (returns_embeddings, state_embeddings, action_embeddings), dim=1\n","        ).permute(0, 2, 1, 3).reshape(B, 3 * T, self.h_dim)\n","\n","        h = self.embed_ln(h)\n","        \n","        # transformer and prediction\n","        h = self.transformer(h)\n","\n","        # get h reshaped such that its size = (B x 3 x T x h_dim) and\n","        # h[:, 0, t] is conditioned on r_0, s_0, a_0 ... r_t\n","        # h[:, 1, t] is conditioned on r_0, s_0, a_0 ... r_t, s_t\n","        # h[:, 2, t] is conditioned on r_0, s_0, a_0 ... r_t, s_t, a_t\n","        h = h.reshape(B, T, 3, self.h_dim).permute(0, 2, 1, 3)\n","\n","        # get predictions\n","        return_preds = self.predict_rtg(h[:,2])     # predict next rtg given r, s, a\n","        state_preds = self.predict_state(h[:,2])    # predict next state given r, s, a\n","        action_preds = self.predict_action(h[:,1])  # predict action given r, s\n","    \n","        return state_preds, action_preds, return_preds\n","\n"],"metadata":{"id":"jVQLWrCzlkH-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","\n","\n","class TrajectoryModel(nn.Module):\n","\n","    def __init__(self, state_dim, act_dim, max_length=None):\n","        super().__init__()\n","\n","        self.state_dim = state_dim\n","        self.act_dim = act_dim\n","        self.max_length = max_length\n","\n","    def forward(self, states, actions, rewards, masks=None, attention_mask=None):\n","        # \"masked\" tokens or unspecified inputs can be passed in as None\n","        return None, None, None\n","\n","    def get_action(self, states, actions, rewards, **kwargs):\n","        # these will come as tensors on the correct device\n","        return torch.zeros_like(actions[-1])\n","\n","\n","class MLPBCModel(TrajectoryModel):\n","\n","    \"\"\"\n","    Simple MLP that predicts next action a from past states s.\n","    \"\"\"\n","\n","    def __init__(self, state_dim, act_dim, hidden_size, n_layer, dropout=0.1, max_length=1, **kwargs):\n","        super().__init__(state_dim, act_dim)\n","\n","        self.hidden_size = hidden_size\n","        self.max_length = max_length\n","\n","        layers = [nn.Linear(max_length*self.state_dim, hidden_size)]\n","        for _ in range(n_layer-1):\n","            layers.extend([\n","                nn.ReLU(),\n","                nn.Dropout(dropout),\n","                nn.Linear(hidden_size, hidden_size)\n","            ])\n","        layers.extend([\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_size, self.act_dim),\n","            nn.Tanh(),\n","        ])\n","\n","        self.model = nn.Sequential(*layers)\n","\n","    def forward(self, states, actions, rewards, attention_mask=None, target_return=None):\n","\n","        states = states[:,-self.max_length:].reshape(states.shape[0], -1)  # concat states\n","        actions = self.model(states).reshape(states.shape[0], 1, self.act_dim)\n","\n","        return None, actions, None\n","\n","    def get_action(self, states, actions, rewards, **kwargs):\n","        states = states.reshape(1, -1, self.state_dim)\n","        if states.shape[1] < self.max_length:\n","            states = torch.cat(\n","                [torch.zeros((1, self.max_length-states.shape[1], self.state_dim),\n","                             dtype=torch.float32, device=states.device), states], dim=1)\n","        states = states.to(dtype=torch.float32)\n","        _, actions, _ = self.forward(states, None, None, **kwargs)\n","        return actions[0,-1]"],"metadata":{"id":"yDPpN-aCrvE5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# train"],"metadata":{"id":"p7AK6T9Picu-"}},{"cell_type":"code","source":["\n","start_time = datetime.now().replace(microsecond=0)\n","\n","start_time_str = start_time.strftime(\"%y-%m-%d-%H-%M-%S\")\n","\n","prefix = \"dt_\" + \"E_\" + env_d4rl_name\n","#prefix = \"dt_\" + env_d4rl_name\n","\n","\n","\n","\n","save_model_name =  prefix + \"_model_\" + \".pt\"\n","save_model_path = os.path.join(log_dir, save_model_name)\n","save_best_model_path = save_model_path[:-3] + \"_best.pt\"\n","\n","\n","log_csv_name = prefix + \"_log_\"  + \".csv\"\n","log_csv_path = os.path.join(log_dir, log_csv_name)\n","\n","\n","csv_writer = csv.writer(open(log_csv_path, 'a', 1))\n","csv_header = ([\"duration\", \"num_updates\", \"action_loss\", \n","               \"eval_avg_reward\", \"eval_avg_ep_len\", \"eval_d4rl_score\"])\n","\n","csv_writer.writerow(csv_header)\n","\n","\n","print(\"=\" * 60)\n","print(\"start time: \" + start_time_str)\n","print(\"=\" * 60)\n","\n","print(\"device set to: \" + str(device))\n","print(\"dataset path: \" + dataset_path)\n","print(\"model save path: \" + save_model_path)\n","print(\"log csv save path: \" + log_csv_path)\n","\n","\n","traj_dataset = D4RLTrajectoryDataset(dataset_path, context_len, rtg_scale)\n","\n","traj_data_loader = DataLoader(traj_dataset,\n","\t\t\t\t\t\tbatch_size=batch_size,\n","\t\t\t\t\t\tshuffle=True,\n","\t\t\t\t\t\tpin_memory=True,\n","\t\t\t\t\t\tdrop_last=True) \n","\n","data_iter = iter(traj_data_loader)\n","\n","## get state stats from dataset\n","state_mean, state_std = traj_dataset.get_state_stats()\n","\n","env = gym.make(env_name)\n","\n","state_dim = env.observation_space.shape[0]\n","act_dim = env.action_space.shape[0]\n","\n","\n","model = DecisionTransformer(\n","\t\t\tstate_dim=state_dim,\n","\t\t\tact_dim=act_dim,\n","\t\t\tn_blocks=n_blocks,\n","\t\t\th_dim=embed_dim,\n","\t\t\tcontext_len=context_len,\n","\t\t\tn_heads=n_heads,\n","\t\t\tdrop_p=dropout_p,\n","\t\t).to(device)\n","\n","\"\"\"\n","model = DecisionInformer\t(\n","\t\t\tstate_dim=state_dim,\n","\t\t\tact_dim=act_dim,\n","\t\t\tn_blocks=n_blocks,\n","\t\t\th_dim=embed_dim,\n","\t\t\tcontext_len=context_len,\n","\t\t\tn_heads=n_heads,\n","\t\t\tdrop_p=dropout_p,\n","\t\t).to(device)\n","\"\"\"\n","\"\"\"\n","model = DecisionReformer\t(\n","\t\t\tstate_dim=state_dim,\n","\t\t\tact_dim=act_dim,\n","\t\t\tn_blocks=n_blocks,\n","\t\t\th_dim=embed_dim,\n","\t\t\tcontext_len=context_len,\n","\t\t\tn_heads=n_heads,\n","\t\t\tdrop_p=dropout_p,\n","\t\t).to(device)\n","\"\"\"\n","\n","\n","optimizer = torch.optim.AdamW(\n","\t\t\t\t\tmodel.parameters(), \n","\t\t\t\t\tlr=lr, \n","\t\t\t\t\tweight_decay=wt_decay\n","\t\t\t\t)\n","\n","scheduler = torch.optim.lr_scheduler.LambdaLR(\n","\t\toptimizer,\n","\t\tlambda steps: min((steps+1)/warmup_steps, 1)\n","\t)\n","\n","max_d4rl_score = -1.0\n","total_updates = 0\n","\n","for i_train_iter in range(max_train_iters):\n","\n","\tlog_action_losses = []\t\n","\tmodel.train()\n"," \n","\tfor _ in range(num_updates_per_iter):\n","\t\ttry:\n","\t\t\ttimesteps, states, actions, returns_to_go, traj_mask = next(data_iter)\n","\t\texcept StopIteration:\n","\t\t\tdata_iter = iter(traj_data_loader)\n","\t\t\ttimesteps, states, actions, returns_to_go, traj_mask = next(data_iter)\n","\n","\t\ttimesteps = timesteps.to(device)\t# B x T\n","\t\tstates = states.to(device)\t\t\t# B x T x state_dim\n","\t\tactions = actions.to(device)\t\t# B x T x act_dim\n","\t\treturns_to_go = returns_to_go.to(device).unsqueeze(dim=-1) # B x T x 1\n","\t\ttraj_mask = traj_mask.to(device)\t# B x T\n","\n","\t\taction_target = torch.clone(actions).detach().to(device)\n","\t\n","\t\tstate_preds, action_preds, return_preds = model.forward(\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\ttimesteps=timesteps,\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\tstates=states,\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\tactions=actions,\n","\t\t\t\t\t\t\t\t\t\t\t\t\t\treturns_to_go=returns_to_go\n","\t\t\t\t\t\t\t\t\t\t\t\t\t)\n","\n","\t\t# only consider non padded elements\n","\t\taction_preds = action_preds.view(-1, act_dim)[traj_mask.view(-1,) > 0]\n","\t\taction_target = action_target.view(-1, act_dim)[traj_mask.view(-1,) > 0]\n","\n","\t\taction_loss = F.mse_loss(action_preds, action_target, reduction='mean')\n","\n","\t\toptimizer.zero_grad()\n","\t\taction_loss.backward()\n","\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n","\t\toptimizer.step()\n","\t\tscheduler.step()\n","\n","\t\tlog_action_losses.append(action_loss.detach().cpu().item())\n","\n","\t# evaluate on env\n","\tresults = evaluate_on_env(model, device, context_len, env, rtg_target, rtg_scale,\n","\t                        num_eval_ep, max_eval_ep_len, state_mean, state_std, \n","\t\t\t\t\t\t\t)\n","\teval_avg_reward = results['eval/avg_reward']\n","\teval_avg_ep_len = results['eval/avg_ep_len']\n","\teval_d4rl_score = get_d4rl_normalized_score(results['eval/avg_reward'], env_name) * 100\n","\n","\tmean_action_loss = np.mean(log_action_losses)\n","\ttime_elapsed = str(datetime.now().replace(microsecond=0) - start_time)\n","\n","\ttotal_updates += num_updates_per_iter\n","\n","\tlog_str = (\"=\" * 60 + '\\n' +\n","\t\t\t\"time elapsed: \" + time_elapsed  + '\\n' +\n","\t\t\t\"num of updates: \" + str(total_updates) + '\\n' +\n","\t\t\t\"action loss: \" +  format(mean_action_loss, \".5f\") + '\\n' +\n","\t\t\t\"eval avg reward: \" + format(eval_avg_reward, \".5f\") + '\\n' +\n","\t\t\t\"eval avg ep len: \" + format(eval_avg_ep_len, \".5f\") + '\\n' +\n","\t\t\t\"eval d4rl score: \" + format(eval_d4rl_score, \".5f\")\n","\t\t\t)\n","\n","\tprint(log_str)\n","\n","\tlog_data = [time_elapsed, total_updates, mean_action_loss,\n","\t\t\t\teval_avg_reward, eval_avg_ep_len,\n","\t\t\t\teval_d4rl_score]\n","\n","\tcsv_writer.writerow(log_data)\n","\t\n","\t# save model\n","\tprint(\"max d4rl score: \" + format(max_d4rl_score, \".5f\"))\n","\tif eval_d4rl_score >= max_d4rl_score:\n","\t\tprint(\"saving max d4rl score model at: \" + save_best_model_path)\n","\t\ttorch.save(model.state_dict(), save_best_model_path)\n","\t\tmax_d4rl_score = eval_d4rl_score\n","\n","\tprint(\"saving current model at: \" + save_model_path)\n","\ttorch.save(model.state_dict(), save_model_path)\n","\n","\n","print(\"=\" * 60)\n","print(\"finished training!\")\n","print(\"=\" * 60)\n","end_time = datetime.now().replace(microsecond=0)\n","time_elapsed = str(end_time - start_time)\n","end_time_str = end_time.strftime(\"%y-%m-%d-%H-%M-%S\")\n","print(\"started training at: \" + start_time_str)\n","print(\"finished training at: \" + end_time_str)\n","print(\"total training time: \" + time_elapsed)\n","print(\"max d4rl score: \" + format(max_d4rl_score, \".5f\"))\n","print(\"saved max d4rl score model at: \" + save_best_model_path)\n","print(\"saved last updated model at: \" + save_model_path)\n","print(\"=\" * 60)\n","\n","csv_writer.close()\n","\n","\n"],"metadata":{"id":"RBLRM5nOVR_8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# test"],"metadata":{"id":"eosqWqRRJLsZ"}},{"cell_type":"code","source":["\n","# set mujoco env path if not already set\n","%env LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/root/.mujoco/mujoco200/bin\n","\n","testdata_csv_path = \"dt_walker_test.csv\"\n","csv_writer = csv.writer(open(testdata_csv_path, 'a', 1))\n","csv_header = ([\"number of test\", \"mean\", \"std\", \"var\"])\n","\n","csv_writer.writerow(csv_header)\n","\n","eval_dataset = \"medium\"\t\t# medium / medium-replay / medium-expert\n","eval_rtg_scale = 1000\t\t# normalize returns to go\n","\n","eval_env_name = \"Walker2d-v3\"\n","eval_rtg_target = 5000\n","eval_env_d4rl_name = f'walker2d-{eval_dataset}-v2'\n","\n","#eval_env_name = \"HalfCheetah-v3\"\n","#eval_rtg_target = 6000\n","#eval_env_d4rl_name = f'halfcheetah-{eval_dataset}-v2'\n","\n","#eval_env_name = \"Hopper-v3\"\n","#eval_rtg_target = 3600\n","#eval_env_d4rl_name = f'hopper-{eval_dataset}-v2'\n","\n","\n","num_test_eval_ep = 10\t\t\t# num of evaluation episodes\n","eval_max_eval_ep_len = 1000\t\t# max len of one episode\n","\n","\n","context_len = 20        # K in decision transformer\n","n_blocks = 3            # num of transformer blocks\n","embed_dim = 128         # embedding (hidden) dim of transformer\n","n_heads = 1             # num of transformer heads\n","dropout_p = 0.1         # dropout probability\n","\n","\n","eval_chk_pt_dir = \"./dt_runs/\"\n","\n","\n","eval_chk_pt_name = \"dt_E_walker2d-medium-v2_model.pt\"\n","eval_chk_pt_list = [eval_chk_pt_name]\n","\n","\n","## manually override check point list\n","## passing a list will evaluate on all checkpoints\n","## and output mean and std score\n","\n","\n","# eval_chk_pt_list = [\n","# \t\n","# ]\n","\n","\n","\n","env_data_stats = get_d4rl_dataset_stats(eval_env_d4rl_name)\n","eval_state_mean = np.array(env_data_stats['state_mean'])\n","eval_state_std = np.array(env_data_stats['state_std'])\n","\n","eval_env = gym.make(eval_env_name)\n","\n","state_dim = eval_env.observation_space.shape[0]\n","act_dim = eval_env.action_space.shape[0]\n","\n","\n","for it in range(30):\n","\teval_env = gym.make(eval_env_name)\n","\n","\tstate_dim = eval_env.observation_space.shape[0]\n","\tact_dim = eval_env.action_space.shape[0]\n","\tall_scores = []\n","\n","\tfor eval_chk_pt_name in eval_chk_pt_list:\n","\n","\t\teval_model = DecisionTransformer(\n","\t\t\t\t\tstate_dim=state_dim,\n","\t\t\t\t\tact_dim=act_dim,\n","\t\t\t\t\tn_blocks=n_blocks,\n","\t\t\t\t\th_dim=embed_dim,\n","\t\t\t\t\tcontext_len=context_len,\n","\t\t\t\t\tn_heads=n_heads,\n","\t\t\t\t\tdrop_p=dropout_p,\n","\t\t\t\t).to(device)\n","\n","\n","\t\teval_chk_pt_path = os.path.join(eval_chk_pt_dir, eval_chk_pt_name)\n","\n","\t\t# load checkpoint\n","\t\teval_model.load_state_dict(torch.load(eval_chk_pt_path, map_location=device))\n","\n","\t\tprint(\"model loaded from: \" + eval_chk_pt_path)\n","\n","\t\t# evaluate on env\n","\t\tresults = evaluate_on_env(eval_model, device, context_len,\n","\t\t\t\t\t\t\t\teval_env, eval_rtg_target, eval_rtg_scale,\n","\t\t\t\t\t\t\t\tnum_test_eval_ep, eval_max_eval_ep_len,\n","\t\t\t\t\t\t\t\teval_state_mean, eval_state_std)\n","\t\tprint(results)\n","\n","\t\tnorm_score = get_d4rl_normalized_score(results['eval/avg_reward'], eval_env_name) * 100\n","\t\tprint(\"normalized d4rl score: \", norm_score)\n","\n","\t\tall_scores.append(norm_score)\n","\n","\t\tprint(\"=\" * 60)\n","\t\tall_scores = np.array(all_scores)\n","\t\tprint(\"evaluated on env: \" + eval_env_name)\n","\t\tprint(\"total num of checkpoints evaluated: \" + str(len(eval_chk_pt_list)))\n","\t\tprint(\"d4rl score mean: \" + format(all_scores.mean(), \".5f\"))\n","\t\tprint(\"d4rl score std: \" + format(all_scores.std(), \".5f\"))\n","\t\tprint(\"d4rl score var: \" + format(all_scores.var(), \".5f\"))\n","\t\tprint(\"=\" * 60)\n","\n","\t\ttest_data = [it,format(all_scores.mean(), \".5f\"),format(all_scores.std(), \".5f\"),\n","\t\t\t\t\t\t\t\tformat(all_scores.var(), \".5f\")]\n","\n","\t\tcsv_writer.writerow(test_data)\n","\n"],"metadata":{"id":"Q4-WPlC1VR3q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# plots"],"metadata":{"id":"DjBsdz9mKbZg"}},{"cell_type":"code","source":["import numpy as np\n","import pickle\n","\n","import pandas as pd\n","import glob\n","\n","import matplotlib.pyplot as plt\n","\n","\n","env_d4rl_name = 'walker2d-medium-v2'\n","\n","log_dir = 'dt_runs/'\n","\n","x_key = \"num_updates\"\n","y_key = \"eval_d4rl_score\"\n","y_smoothing_win = 5\n","plot_avg = False\n","save_fig = False\n","\n","if plot_avg:\n","    save_fig_path = env_d4rl_name + \"_avg.png\"\n","else:\n","    save_fig_path = env_d4rl_name + \".png\"\n","\n","\n","all_files = glob.glob(log_dir + f'/dt_{env_d4rl_name}*.csv')\n","\n","ax = plt.gca()\n","ax.set_title(env_d4rl_name)\n","\n","if plot_avg:\n","    name_list = []\n","    df_list = []\n","    for filename in all_files:\n","        frame = pd.read_csv(filename, index_col=None, header=0)\n","        print(filename, frame.shape)\n","        frame['y_smooth'] = frame[y_key].rolling(window=y_smoothing_win).mean() \n","        df_list.append(frame)\n","    \n","    \n","    df_concat = pd.concat(df_list)\n","    df_concat_groupby = df_concat.groupby(df_concat.index)\n","    data_avg = df_concat_groupby.mean()\n","\n","    data_avg.plot(x=x_key, y='y_smooth', ax=ax)\n","    \n","    ax.set_xlabel(x_key)\n","    ax.set_ylabel(y_key)\n","    ax.legend(['avg of all runs'], loc='lower right')\n","    \n","    if save_fig:\n","        plt.savefig(save_fig_path)\n","        \n","    plt.show()\n","    \n","    \n","else:\n","    name_list = []\n","    for filename in all_files:\n","        frame = pd.read_csv(filename, index_col=None, header=0)\n","        print(filename, frame.shape)\n","        frame['y_smooth'] = frame[y_key].rolling(window=y_smoothing_win).mean()\n","        frame.plot(x=x_key, y='y_smooth', ax=ax)\n","        name_list.append(filename.split('/')[-1])\n","    \n","    ax.set_xlabel(x_key)\n","    ax.set_ylabel(y_key)\n","    ax.legend(name_list, loc='lower right')\n","    \n","    if save_fig:\n","        plt.savefig(save_fig_path)\n","    \n","    plt.show()\n","    \n"],"metadata":{"id":"2WM69ti2KaRN"},"execution_count":null,"outputs":[]}]}